{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04ff146",
   "metadata": {},
   "source": [
    "# Detecção de Outliers em Cybersecurity: Análise de Ameaças usando Aprendizado Não Supervisionado\n",
    "\n",
    "## Resumo Executivo\n",
    "\n",
    "Este notebook implementa e avalia técnicas de aprendizado não supervisionado para detecção de ameaças cibernéticas através da identificação de outliers. O objetivo principal é validar se os outliers detectados pelos algoritmos correspondem efetivamente a agentes maliciosos no dataset Text-Based Cyber Threat Detection.\n",
    "\n",
    "### Objetivos:\n",
    "1. Aplicar técnicas de clustering para identificar padrões nos dados\n",
    "2. Utilizar algoritmos de detecção de outliers (Isolation Forest, LOF, One-Class SVM)\n",
    "3. Validar os outliers detectados contra labels reais de ameaças\n",
    "4. Comparar a eficácia dos diferentes métodos\n",
    "\n",
    "### Metodologia:\n",
    "- Dataset: Text-Based Cyber Threat Detection (Kaggle)\n",
    "- Técnicas: Clustering (K-Means, DBSCAN) + Detecção de Outliers (IF, LOF, OC-SVM)\n",
    "- Validação: Métricas de classificação (Precision, Recall, F1-Score)\n",
    "\n",
    "---\n",
    "\n",
    "**Projeto**: Mitigação de Ataques por Envenenamento em Aprendizado Federado  \n",
    "**Instituição**: Faculdade Impacta  \n",
    "**Data**: Outubro 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe33512",
   "metadata": {},
   "source": [
    "## 1. Setup e Configuração do Ambiente\n",
    "\n",
    "Importação de bibliotecas necessárias e configuração do ambiente de visualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab453e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas padrão\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn: Pré-processamento\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scikit-learn: Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Scikit-learn: Detecção de Outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Scikit-learn: Métricas de avaliação\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configurar visualizações\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Criar diretório de saída para imagens\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), 'output_images')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Ambiente configurado com sucesso!\")\n",
    "print(f\"Diretório de saída: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887960f6",
   "metadata": {},
   "source": [
    "[## 2. Carregamento e Exploração Inicial dos Dados\n",
    "\n",
    "IMPORTANTE: Este notebook assume que você baixou o dataset do Kaggle.  \n",
    "Dataset: https://www.kaggle.com/datasets/ramoliyafenil/text-based-cyber-threat-detection\n",
    "\n",
    "Coloque o arquivo CSV na pasta `data/` do projeto., ## 3. Análise Exploratória dos Dados, ## 4. Pré-processamento e Feature Engineering, ## 5. Detecção de Outliers - Múltiplas Técnicas, ## 6. Validação: Comparação com Labels Reais, ## 7. Visualização de Resultados, ## 8. Avaliação e Conclusões]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ad3bd",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Exploração Inicial dos Dados\n",
    "\n",
    "O dataset foi baixado do Kaggle usando kagglehub e está localizado em `../../data/cyber-outlier-detection/`.\n",
    "\n",
    "**Dataset**: Text-Based Cyber Threat Detection  \n",
    "**Amostras**: 19,940 registros  \n",
    "**Features**: 10 colunas (text, entities, relations, labels, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset principal\n",
    "data_path = '../../data/cyber-outlier-detection/cyber-threat-intelligence_all.csv'\n",
    "\n",
    "print(\"Carregando dataset...\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"✓ Dataset carregado com sucesso!\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Memória: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Visualizar primeiras linhas\n",
    "print(\"\\nPrimeiras 5 linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações gerais do dataset\n",
    "print(\"=\"*80)\n",
    "print(\"ANÁLISE EXPLORATÓRIA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. INFORMAÇÕES GERAIS:\")\n",
    "print(f\"   Total de registros: {len(df):,}\")\n",
    "print(f\"   Total de colunas: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n2. COLUNAS E TIPOS:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    non_null = df[col].notna().sum()\n",
    "    null_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "    print(f\"   {i:2d}. {col:20s} - {str(df[col].dtype):10s} ({non_null:5d} não-nulos, {null_pct:5.1f}% nulos)\")\n",
    "\n",
    "print(\"\\n3. ESTATÍSTICAS DAS COLUNAS PRINCIPAIS:\")\n",
    "\n",
    "# Coluna text\n",
    "if 'text' in df.columns:\n",
    "    print(f\"\\n   TEXT:\")\n",
    "    print(f\"      Textos únicos: {df['text'].nunique():,}\")\n",
    "    print(f\"      Comprimento médio: {df['text'].str.len().mean():.0f} caracteres\")\n",
    "    print(f\"      Comprimento mín/máx: {df['text'].str.len().min()}/{df['text'].str.len().max()}\")\n",
    "\n",
    "# Coluna label\n",
    "if 'label' in df.columns:\n",
    "    print(f\"\\n   LABELS:\")\n",
    "    print(f\"      Labels únicos: {df['label'].nunique()}\")\n",
    "    print(f\"      Labels não-nulos: {df['label'].notna().sum():,} ({(df['label'].notna().sum()/len(df))*100:.1f}%)\")\n",
    "    print(f\"\\n      Top 10 categorias:\")\n",
    "    print(df['label'].value_counts().head(10).to_string())\n",
    "\n",
    "# Coluna entities\n",
    "if 'entities' in df.columns:\n",
    "    print(f\"\\n   ENTITIES:\")\n",
    "    non_null_entities = df['entities'].notna().sum()\n",
    "    print(f\"      Registros com entities: {non_null_entities:,} ({(non_null_entities/len(df))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c5b3e",
   "metadata": {},
   "source": [
    "## 3. Visualizações Exploratórias\n",
    "\n",
    "Vamos criar visualizações para entender melhor a distribuição dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47130a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar visualizações exploratórias\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribuição de comprimento de texto\n",
    "ax1 = axes[0, 0]\n",
    "text_lengths = df['text'].str.len()\n",
    "ax1.hist(text_lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Comprimento do Texto (caracteres)', fontsize=12)\n",
    "ax1.set_ylabel('Frequência', fontsize=12)\n",
    "ax1.set_title('Distribuição do Comprimento dos Textos', fontsize=14, fontweight='bold')\n",
    "ax1.axvline(text_lengths.mean(), color='red', linestyle='--', linewidth=2, label=f'Média: {text_lengths.mean():.0f}')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Top 15 labels (excluindo nulos)\n",
    "ax2 = axes[0, 1]\n",
    "df_labels = df['label'].dropna()\n",
    "top_labels = df_labels.value_counts().head(15)\n",
    "bars = ax2.barh(range(len(top_labels)), top_labels.values, color='coral', edgecolor='black')\n",
    "ax2.set_yticks(range(len(top_labels)))\n",
    "ax2.set_yticklabels(top_labels.index, fontsize=10)\n",
    "ax2.set_xlabel('Contagem', fontsize=12)\n",
    "ax2.set_title('Top 15 Categorias de Labels', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (bar, value) in enumerate(zip(bars, top_labels.values)):\n",
    "    ax2.text(value + 20, i, f'{value}', va='center', fontsize=9)\n",
    "\n",
    "# 3. Distribuição de valores nulos\n",
    "ax3 = axes[1, 0]\n",
    "null_counts = df.isnull().sum()\n",
    "null_pct = (null_counts / len(df)) * 100\n",
    "columns_with_nulls = null_pct[null_pct > 0].sort_values(ascending=True)\n",
    "\n",
    "if len(columns_with_nulls) > 0:\n",
    "    bars = ax3.barh(range(len(columns_with_nulls)), columns_with_nulls.values, color='salmon', edgecolor='black')\n",
    "    ax3.set_yticks(range(len(columns_with_nulls)))\n",
    "    ax3.set_yticklabels(columns_with_nulls.index, fontsize=10)\n",
    "    ax3.set_xlabel('Percentual de Valores Nulos (%)', fontsize=12)\n",
    "    ax3.set_title('Valores Nulos por Coluna', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (bar, value) in enumerate(zip(bars, columns_with_nulls.values)):\n",
    "        ax3.text(value + 1, i, f'{value:.1f}%', va='center', fontsize=9)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Sem valores nulos', ha='center', va='center', fontsize=14)\n",
    "    ax3.set_xlim(0, 1)\n",
    "    ax3.set_ylim(0, 1)\n",
    "\n",
    "# 4. Proporção de registros com/sem labels\n",
    "ax4 = axes[1, 1]\n",
    "label_status = df['label'].notna().value_counts()\n",
    "colors = ['#66c2a5', '#fc8d62']\n",
    "explode = (0.05, 0)\n",
    "wedges, texts, autotexts = ax4.pie(label_status.values, \n",
    "                                     labels=['Com Label', 'Sem Label'],\n",
    "                                     colors=colors,\n",
    "                                     autopct='%1.1f%%',\n",
    "                                     startangle=90,\n",
    "                                     explode=explode,\n",
    "                                     textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "ax4.set_title('Proporção de Registros com/sem Labels', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '01_exploratory_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualização salva: {OUTPUT_DIR}/01_exploratory_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c81f1",
   "metadata": {},
   "source": [
    "## 4. Pré-processamento e Feature Engineering\n",
    "\n",
    "Para aplicar técnicas de detecção de outliers, precisamos converter o texto em features numéricas. Vamos usar:\n",
    "1. **TF-IDF** para vetorizar o texto\n",
    "2. **PCA** para reduzir dimensionalidade\n",
    "3. **StandardScaler** para normalizar features\n",
    "\n",
    "### 4.1. Preparação dos Dados com Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar apenas registros com labels para validação\n",
    "df_labeled = df[df['label'].notna()].copy()\n",
    "\n",
    "print(f\"Dataset com labels: {len(df_labeled):,} registros\")\n",
    "print(f\"Dataset completo: {len(df):,} registros\")\n",
    "print(f\"Percentual com labels: {(len(df_labeled)/len(df))*100:.1f}%\")\n",
    "\n",
    "# Criar labels binários: threat (malware, attack-pattern, threat-actor, vulnerability) vs normal\n",
    "threat_labels = ['malware', 'attack-pattern', 'threat-actor', 'vulnerability', 'tools']\n",
    "\n",
    "df_labeled['is_threat'] = df_labeled['label'].apply(\n",
    "    lambda x: 1 if str(x).lower() in threat_labels else 0\n",
    ")\n",
    "\n",
    "print(f\"\\nDistribuição de ameaças:\")\n",
    "print(f\"  Threats (1): {(df_labeled['is_threat'] == 1).sum():,} ({(df_labeled['is_threat'] == 1).sum()/len(df_labeled)*100:.1f}%)\")\n",
    "print(f\"  Normal  (0): {(df_labeled['is_threat'] == 0).sum():,} ({(df_labeled['is_threat'] == 0).sum()/len(df_labeled)*100:.1f}%)\")\n",
    "\n",
    "# Amostra representativa para análise (para performance)\n",
    "# Vamos usar uma amostra estratificada de 15,000 registros\n",
    "sample_size = min(15000, len(df_labeled))\n",
    "df_sample = df_labeled.sample(n=sample_size, random_state=42, stratify=df_labeled['is_threat'])\n",
    "\n",
    "print(f\"\\nUsando amostra de {len(df_sample):,} registros para análise\")\n",
    "print(f\"  Threats: {(df_sample['is_threat'] == 1).sum():,}\")\n",
    "print(f\"  Normal:  {(df_sample['is_threat'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7998709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f584e5e",
   "metadata": {},
   "source": [
    "### 4.2. Vetorização TF-IDF\n",
    "\n",
    "Converter textos em vetores numéricos usando TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VETORIZAÇÃO TF-IDF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TF-IDF Vectorizer com parâmetros ajustados\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500,           # Top 500 features mais importantes\n",
    "    min_df=5,                   # Palavra deve aparecer em no mínimo 5 documentos\n",
    "    max_df=0.7,                 # Ignorar palavras que aparecem em mais de 70% dos docs\n",
    "    ngram_range=(1, 2),         # Unigramas e bigramas\n",
    "    stop_words='english',       # Remover stopwords\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "# Aplicar vetorização\n",
    "print(\"\\nVetorizando textos...\")\n",
    "X_tfidf = vectorizer.fit_transform(df_sample['text'])\n",
    "\n",
    "print(f\"✓ Vetorização completa!\")\n",
    "print(f\"  Shape da matriz TF-IDF: {X_tfidf.shape}\")\n",
    "print(f\"  Total de features: {X_tfidf.shape[1]:,}\")\n",
    "print(f\"  Sparsity: {(1.0 - X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Converter para array denso\n",
    "X_tfidf_dense = X_tfidf.toarray()\n",
    "\n",
    "# Labels verdadeiros\n",
    "y_true = df_sample['is_threat'].values\n",
    "\n",
    "print(f\"\\n  Labels shape: {y_true.shape}\")\n",
    "print(f\"  Threats (1): {(y_true == 1).sum():,}\")\n",
    "print(f\"  Normal (0):  {(y_true == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17dabc",
   "metadata": {},
   "source": [
    "### 4.3. Redução de Dimensionalidade com PCA\n",
    "\n",
    "Reduzir de 500 features para 50 componentes principais (mantendo ~90% da variância)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3870594",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"REDUÇÃO DE DIMENSIONALIDADE COM PCA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# PCA com 50 componentes\n",
    "n_components = 50\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "print(f\"\\nAplicando PCA ({n_components} componentes)...\")\n",
    "X_pca = pca.fit_transform(X_tfidf_dense)\n",
    "\n",
    "# Calcular variância explicada\n",
    "explained_var = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"✓ PCA completa!\")\n",
    "print(f\"  Shape após PCA: {X_pca.shape}\")\n",
    "print(f\"  Variância explicada: {explained_var*100:.2f}%\")\n",
    "print(f\"  Redução de dimensionalidade: {X_tfidf_dense.shape[1]} → {X_pca.shape[1]} features\")\n",
    "\n",
    "# Normalizar dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "print(f\"\\n✓ Dados normalizados com StandardScaler\")\n",
    "print(f\"  Mean: {X_scaled.mean():.2e}\")\n",
    "print(f\"  Std:  {X_scaled.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0efeb",
   "metadata": {},
   "source": [
    "## 5. Detecção de Outliers - Múltiplas Técnicas\n",
    "\n",
    "Vamos aplicar 5 técnicas diferentes de detecção de outliers e comparar os resultados:\n",
    "\n",
    "1. **Isolation Forest** - Detecta anomalias isolando observações\n",
    "2. **Local Outlier Factor (LOF)** - Baseado em densidade local\n",
    "3. **One-Class SVM** - SVM com kernel RBF para detecção de outliers\n",
    "4. **Elliptic Envelope** - Assume distribuição gaussiana multivariada\n",
    "5. **DBSCAN** - Clustering baseado em densidade (pontos em regiões de baixa densidade são outliers)\n",
    "\n",
    "### 5.1. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4cc168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DETECÇÃO DE OUTLIERS - MÚLTIPLAS TÉCNICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dicionário para armazenar resultados\n",
    "outlier_predictions = {}\n",
    "execution_times = {}\n",
    "\n",
    "# Percentual de contaminação esperado (baseado na proporção de threats)\n",
    "contamination = (y_true == 1).sum() / len(y_true)\n",
    "print(f\"\\nContaminação estimada (threats): {contamination:.3f} ({contamination*100:.1f}%)\\n\")\n",
    "\n",
    "import time\n",
    "\n",
    "# 1. ISOLATION FOREST\n",
    "print(\"1. Isolation Forest...\")\n",
    "start = time.time()\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=contamination,\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    n_jobs=-1\n",
    ")\n",
    "pred_iso = iso_forest.fit_predict(X_scaled)\n",
    "outlier_predictions['Isolation Forest'] = (pred_iso == -1).astype(int)\n",
    "execution_times['Isolation Forest'] = time.time() - start\n",
    "print(f\"   ✓ Outliers detectados: {(pred_iso == -1).sum():,} ({(pred_iso == -1).sum()/len(pred_iso)*100:.1f}%)\")\n",
    "print(f\"   ✓ Tempo: {execution_times['Isolation Forest']:.2f}s\")\n",
    "\n",
    "# 2. LOCAL OUTLIER FACTOR\n",
    "print(\"\\n2. Local Outlier Factor (LOF)...\")\n",
    "start = time.time()\n",
    "lof = LocalOutlierFactor(\n",
    "    contamination=contamination,\n",
    "    n_neighbors=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "pred_lof = lof.fit_predict(X_scaled)\n",
    "outlier_predictions['LOF'] = (pred_lof == -1).astype(int)\n",
    "execution_times['LOF'] = time.time() - start\n",
    "print(f\"   ✓ Outliers detectados: {(pred_lof == -1).sum():,} ({(pred_lof == -1).sum()/len(pred_lof)*100:.1f}%)\")\n",
    "print(f\"   ✓ Tempo: {execution_times['LOF']:.2f}s\")\n",
    "\n",
    "# 3. ONE-CLASS SVM\n",
    "print(\"\\n3. One-Class SVM...\")\n",
    "start = time.time()\n",
    "oc_svm = OneClassSVM(\n",
    "    nu=contamination,\n",
    "    kernel='rbf',\n",
    "    gamma='auto'\n",
    ")\n",
    "pred_ocsvm = oc_svm.fit_predict(X_scaled)\n",
    "outlier_predictions['One-Class SVM'] = (pred_ocsvm == -1).astype(int)\n",
    "execution_times['One-Class SVM'] = time.time() - start\n",
    "print(f\"   ✓ Outliers detectados: {(pred_ocsvm == -1).sum():,} ({(pred_ocsvm == -1).sum()/len(pred_ocsvm)*100:.1f}%)\")\n",
    "print(f\"   ✓ Tempo: {execution_times['One-Class SVM']:.2f}s\")\n",
    "\n",
    "# 4. ELLIPTIC ENVELOPE\n",
    "print(\"\\n4. Elliptic Envelope...\")\n",
    "start = time.time()\n",
    "ee = EllipticEnvelope(\n",
    "    contamination=contamination,\n",
    "    random_state=42,\n",
    "    support_fraction=0.9\n",
    ")\n",
    "pred_ee = ee.fit_predict(X_scaled)\n",
    "outlier_predictions['Elliptic Envelope'] = (pred_ee == -1).astype(int)\n",
    "execution_times['Elliptic Envelope'] = time.time() - start\n",
    "print(f\"   ✓ Outliers detectados: {(pred_ee == -1).sum():,} ({(pred_ee == -1).sum()/len(pred_ee)*100:.1f}%)\")\n",
    "print(f\"   ✓ Tempo: {execution_times['Elliptic Envelope']:.2f}s\")\n",
    "\n",
    "# 5. DBSCAN\n",
    "print(\"\\n5. DBSCAN (Clustering + Outliers)...\")\n",
    "start = time.time()\n",
    "dbscan = DBSCAN(\n",
    "    eps=3.0,\n",
    "    min_samples=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "pred_dbscan = dbscan.fit_predict(X_scaled)\n",
    "outlier_predictions['DBSCAN'] = (pred_dbscan == -1).astype(int)\n",
    "execution_times['DBSCAN'] = time.time() - start\n",
    "n_clusters = len(set(pred_dbscan)) - (1 if -1 in pred_dbscan else 0)\n",
    "print(f\"   ✓ Clusters encontrados: {n_clusters}\")\n",
    "print(f\"   ✓ Outliers detectados: {(pred_dbscan == -1).sum():,} ({(pred_dbscan == -1).sum()/len(pred_dbscan)*100:.1f}%)\")\n",
    "print(f\"   ✓ Tempo: {execution_times['DBSCAN']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ TODAS AS TÉCNICAS EXECUTADAS COM SUCESSO!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f17b5",
   "metadata": {},
   "source": [
    "## 6. Validação: Comparação com Labels Reais\n",
    "\n",
    "Agora vamos validar se os outliers detectados correspondem às ameaças reais (threats) no dataset.\n",
    "\n",
    "**Hipótese**: Outliers detectados = Ameaças cibernéticas (malware, ataques, vulnerabilidades)\n",
    "\n",
    "Métricas usadas:\n",
    "- **Accuracy**: % de predições corretas\n",
    "- **Precision**: % de outliers detectados que são realmente threats\n",
    "- **Recall**: % de threats que foram detectados como outliers\n",
    "- **F1-Score**: Média harmônica entre Precision e Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1de702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VALIDAÇÃO CONTRA LABELS REAIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calcular métricas para cada método\n",
    "results = []\n",
    "\n",
    "for method_name, y_pred in outlier_predictions.items():\n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    results.append({\n",
    "        'Método': method_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'TN': tn,\n",
    "        'FN': fn,\n",
    "        'Tempo (s)': execution_times[method_name]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "    print(f\"  Precision: {precision*100:.2f}%\")\n",
    "    print(f\"  Recall:    {recall*100:.2f}%\")\n",
    "    print(f\"  F1-Score:  {f1*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix: TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANKING DOS MÉTODOS (por F1-Score):\")\n",
    "print(\"=\"*80)\n",
    "print(df_results[['Método', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Tempo (s)']].to_string(index=False))\n",
    "\n",
    "# Salvar resultados\n",
    "results_path = os.path.join(OUTPUT_DIR, 'evaluation_metrics.csv')\n",
    "df_results.to_csv(results_path, index=False)\n",
    "print(f\"\\n✓ Resultados salvos: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaaa2f2",
   "metadata": {},
   "source": [
    "## 7. Visualização de Resultados\n",
    "\n",
    "### 7.1. Comparação de Métricas entre Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização comparativa de métricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Ordenar por métrica atual\n",
    "    df_sorted = df_results.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Criar barras horizontais\n",
    "    bars = ax.barh(range(len(df_sorted)), df_sorted[metric]*100, color=colors[idx], edgecolor='black', alpha=0.8)\n",
    "    ax.set_yticks(range(len(df_sorted)))\n",
    "    ax.set_yticklabels(df_sorted['Método'], fontsize=11)\n",
    "    ax.set_xlabel(f'{metric} (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} por Método', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, (bar, value) in enumerate(zip(bars, df_sorted[metric]*100)):\n",
    "        ax.text(value + 1, i, f'{value:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Destacar melhor método\n",
    "    best_idx = df_sorted[metric].idxmax()\n",
    "    bars[list(df_sorted.index).index(best_idx)].set_edgecolor('gold')\n",
    "    bars[list(df_sorted.index).index(best_idx)].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '02_comparative_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualização salva: {OUTPUT_DIR}/02_comparative_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c1c6a",
   "metadata": {},
   "source": [
    "### 7.2. Matrizes de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416db993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matrizes de confusão\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (method_name, y_pred) in enumerate(outlier_predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Calcular matriz de confusão\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plotar heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n",
    "                square=True, linewidths=2, linecolor='black',\n",
    "                annot_kws={'size': 14, 'weight': 'bold'})\n",
    "    \n",
    "    ax.set_xlabel('Predito', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Real', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{method_name}\\n(F1: {df_results[df_results[\"Método\"]==method_name][\"F1-Score\"].values[0]*100:.1f}%)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.set_xticklabels(['Normal', 'Threat'], fontsize=11)\n",
    "    ax.set_yticklabels(['Normal', 'Threat'], fontsize=11, rotation=0)\n",
    "\n",
    "# Remover subplot extra\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '04_confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualização salva: {OUTPUT_DIR}/04_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dddd6de",
   "metadata": {},
   "source": [
    "### 7.3. Comparação de Outliers Detectados vs Ameaças Reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ddde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparação visual de outliers detectados vs ameaças reais\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "methods = list(outlier_predictions.keys())\n",
    "threats_real = (y_true == 1).sum()\n",
    "outliers_detected = [pred.sum() for pred in outlier_predictions.values()]\n",
    "true_positives = [((y_true == 1) & (pred == 1)).sum() for pred in outlier_predictions.values()]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, [threats_real]*len(methods), width, label='Ameaças Reais', color='#e74c3c', edgecolor='black', alpha=0.8)\n",
    "bars2 = ax.bar(x, outliers_detected, width, label='Outliers Detectados', color='#3498db', edgecolor='black', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, true_positives, width, label='True Positives', color='#2ecc71', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Método de Detecção', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Número de Amostras', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Comparação: Outliers Detectados vs Ameaças Reais', fontsize=15, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods, rotation=20, ha='right', fontsize=11)\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '03_outliers_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualização salva: {OUTPUT_DIR}/03_outliers_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d51f",
   "metadata": {},
   "source": [
    "## 8. Conclusões e Análise dos Resultados\n",
    "\n",
    "### 8.1. Sumário Executivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ANÁLISE FINAL DOS RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar melhor método por cada métrica\n",
    "best_methods = {\n",
    "    'Accuracy': df_results.loc[df_results['Accuracy'].idxmax()],\n",
    "    'Precision': df_results.loc[df_results['Precision'].idxmax()],\n",
    "    'Recall': df_results.loc[df_results['Recall'].idxmax()],\n",
    "    'F1-Score': df_results.loc[df_results['F1-Score'].idxmax()]\n",
    "}\n",
    "\n",
    "print(\"\\n🏆 MELHORES MÉTODOS POR MÉTRICA:\")\n",
    "print(\"-\" * 80)\n",
    "for metric, row in best_methods.items():\n",
    "    print(f\"{metric:12s}: {row['Método']:20s} ({row[metric]*100:.2f}%)\")\n",
    "\n",
    "# Análise geral\n",
    "print(\"\\n\\n📊 ANÁLISE COMPARATIVA:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "best_overall = df_results.iloc[0]\n",
    "print(f\"\\n✅ MELHOR MÉTODO GERAL: {best_overall['Método']}\")\n",
    "print(f\"   • F1-Score: {best_overall['F1-Score']*100:.2f}%\")\n",
    "print(f\"   • Accuracy: {best_overall['Accuracy']*100:.2f}%\")\n",
    "print(f\"   • Precision: {best_overall['Precision']*100:.2f}%\")\n",
    "print(f\"   • Recall: {best_overall['Recall']*100:.2f}%\")\n",
    "print(f\"   • Tempo de execução: {best_overall['Tempo (s)']:.2f}s\")\n",
    "\n",
    "worst_overall = df_results.iloc[-1]\n",
    "print(f\"\\n❌ PIOR MÉTODO: {worst_overall['Método']}\")\n",
    "print(f\"   • F1-Score: {worst_overall['F1-Score']*100:.2f}%\")\n",
    "\n",
    "# Insights\n",
    "print(\"\\n\\n💡 INSIGHTS PRINCIPAIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n1. EFICÁCIA NA DETECÇÃO DE AMEAÇAS:\")\n",
    "avg_precision = df_results['Precision'].mean()\n",
    "avg_recall = df_results['Recall'].mean()\n",
    "print(f\"   • Precision média: {avg_precision*100:.2f}%\")\n",
    "print(f\"   • Recall médio: {avg_recall*100:.2f}%\")\n",
    "\n",
    "if avg_precision > 0.5:\n",
    "    print(\"   → Boa capacidade de identificar ameaças reais entre os outliers detectados\")\n",
    "else:\n",
    "    print(\"   → Muitos falsos positivos - outliers não correspondem a ameaças\")\n",
    "\n",
    "if avg_recall > 0.5:\n",
    "    print(\"   → Alta cobertura - maioria das ameaças foi detectada como outlier\")\n",
    "else:\n",
    "    print(\"   → Baixa cobertura - muitas ameaças não foram detectadas\")\n",
    "\n",
    "print(\"\\n2. TRADE-OFF PRECISION vs RECALL:\")\n",
    "precision_range = df_results['Precision'].max() - df_results['Precision'].min()\n",
    "recall_range = df_results['Recall'].max() - df_results['Recall'].min()\n",
    "print(f\"   • Variação Precision: {precision_range*100:.2f}%\")\n",
    "print(f\"   • Variação Recall: {recall_range*100:.2f}%\")\n",
    "print(\"   → Alguns métodos priorizam precision, outros recall\")\n",
    "\n",
    "print(\"\\n3. PERFORMANCE COMPUTACIONAL:\")\n",
    "fastest = df_results.loc[df_results['Tempo (s)'].idxmin()]\n",
    "slowest = df_results.loc[df_results['Tempo (s)'].idxmax()]\n",
    "print(f\"   • Mais rápido: {fastest['Método']} ({fastest['Tempo (s)']:.2f}s)\")\n",
    "print(f\"   • Mais lento: {slowest['Método']} ({slowest['Tempo (s)']:.2f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ ANÁLISE COMPLETA!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nResultados salvos em: {OUTPUT_DIR}/\")\n",
    "print(f\"  • 01_exploratory_analysis.png\")\n",
    "print(f\"  • 02_comparative_metrics.png\")\n",
    "print(f\"  • 03_outliers_comparison.png\")\n",
    "print(f\"  • 04_confusion_matrices.png\")\n",
    "print(f\"  • evaluation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfd0d2",
   "metadata": {},
   "source": [
    "### 8.2. Considerações Finais\n",
    "\n",
    "**Validação da Hipótese:**\n",
    "- A análise demonstrou que técnicas de detecção de outliers podem ser eficazes para identificar ameaças cibernéticas em dados textuais\n",
    "- Diferentes métodos apresentam diferentes trade-offs entre precision e recall\n",
    "- Elliptic Envelope mostrou-se particularmente eficaz para este tipo de dado\n",
    "\n",
    "**Aplicabilidade em Aprendizado Federado:**\n",
    "- Estas técnicas podem ser usadas para detectar agentes maliciosos em ambientes federados\n",
    "- A detecção de outliers pode identificar participantes que enviam atualizações suspeitas\n",
    "- Importante considerar o equilíbrio entre detecção de ameaças e falsos positivos\n",
    "\n",
    "**Próximos Passos:**\n",
    "1. Testar com diferentes tamanhos de dataset\n",
    "2. Avaliar em cenários de aprendizado federado real\n",
    "3. Implementar ensembles de múltiplos métodos\n",
    "4. Ajustar hiperparâmetros para otimizar F1-Score\n",
    "\n",
    "---\n",
    "\n",
    "**Referências:**\n",
    "- Dataset: Text-Based Cyber Threat Detection (Kaggle)\n",
    "- Scikit-learn Documentation\n",
    "- Projeto: Mitigação de Ataques por Envenenamento em Aprendizado Federado"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
