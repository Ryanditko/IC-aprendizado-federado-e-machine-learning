# penguin_unsupervised.py - Aprendizado N√£o Supervisionado com Dataset Penguin

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.manifold import TSNE

# Suprimir warnings desnecess√°rios
warnings.filterwarnings('ignore')

def get_penguin_path():
    """Retorna o caminho completo para o arquivo penguins.csv"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(base_dir, "penguins.csv")

def load_penguin_dataset():
    """Carrega o dataset penguin de diferentes fontes"""
    print("üîç Procurando dataset penguin...")
    
    penguin_path = get_penguin_path()
    
    # Tentar carregar de arquivo local primeiro
    if os.path.exists(penguin_path):
        print("‚úÖ Arquivo local encontrado!")
        return pd.read_csv(penguin_path)
    
    # Tentar via seaborn
    try:
        print("üì• Tentando baixar via seaborn...")
        penguin = sns.load_dataset("penguins")
        if penguin is not None and not penguin.empty:
            # Salvar para uso futuro
            penguin.to_csv(penguin_path, index=False)
            print("‚úÖ Dataset baixado via seaborn e salvo localmente!")
            return penguin
    except Exception as e:
        print(f"‚ùå Erro no seaborn: {e}")
    
    # Tentar via URL direta
    try:
        print("üì• Tentando baixar via URL...")
        url = "https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv"
        penguin = pd.read_csv(url)
        # Salvar para uso futuro
        penguin.to_csv(penguin_path, index=False)
        print("‚úÖ Dataset baixado via URL e salvo localmente!")
        return penguin
    except Exception as e:
        print(f"‚ùå Erro no download: {e}")
    
    print("‚ùå N√£o foi poss√≠vel carregar o dataset!")
    return None

def clean_and_prepare_data(data):
    """Limpa e prepara os dados para an√°lise"""
    print("\n" + "="*50)
    print("üßπ LIMPEZA E PREPARA√á√ÉO DOS DADOS")
    print("="*50)
    
    print(f"üìä Dados originais: {data.shape}")
    print(f"‚ùì Valores faltantes por coluna:")
    missing_before = data.isnull().sum()
    print(missing_before[missing_before > 0])
    
    # Fazer uma c√≥pia para n√£o modificar o original
    data_clean = data.copy()
    
    # Remover linhas com muitos valores faltantes
    data_clean = data_clean.dropna()
    
    print(f"\nüìä Dados ap√≥s limpeza: {data_clean.shape}")
    print(f"‚úÖ Removidas {data.shape[0] - data_clean.shape[0]} linhas com valores faltantes")
    
    # Codificar vari√°veis categ√≥ricas se necess√°rio
    categorical_columns = data_clean.select_dtypes(include=['object']).columns
    label_encoders = {}
    
    if len(categorical_columns) > 0:
        print(f"\nüè∑Ô∏è Codificando vari√°veis categ√≥ricas: {list(categorical_columns)}")
        for col in categorical_columns:
            if col != 'species':  # Manter species para compara√ß√£o
                le = LabelEncoder()
                data_clean[col] = le.fit_transform(data_clean[col])
                label_encoders[col] = le
    
    return data_clean, label_encoders

def explore_data(data):
    """An√°lise explorat√≥ria dos dados"""
    print("\n" + "="*50)
    print("üìä AN√ÅLISE EXPLORAT√ìRIA DOS DADOS")
    print("="*50)
    
    print(f"\nüìã Informa√ß√µes gerais:")
    print(f"‚Ä¢ N√∫mero de amostras: {data.shape[0]}")
    print(f"‚Ä¢ N√∫mero de features: {data.shape[1] - 1}")  # -1 para excluir a coluna target
    
    print(f"\nüìà Estat√≠sticas descritivas:")
    features = data.select_dtypes(include=[np.number])
    print(features.describe())
    
    print(f"\nüéØ Distribui√ß√£o das esp√©cies (para refer√™ncia):")
    if 'species' in data.columns:
        print(data['species'].value_counts())
    
    # Verificar valores missing
    print(f"\n‚ùì Valores faltantes:")
    missing = data.isnull().sum()
    if missing.sum() > 0:
        print(missing[missing > 0])
    else:
        print("N√£o h√° valores faltantes!")

def visualize_data(data):
    """Visualiza√ß√µes dos dados"""
    print("\n" + "="*50)
    print("üìä VISUALIZA√á√ïES DOS DADOS")
    print("="*50)
    
    # Separar features num√©ricas
    features = data.select_dtypes(include=[np.number])
    
    # 1. Matriz de correla√ß√£o
    plt.figure(figsize=(15, 12))
    plt.subplot(3, 2, 1)
    correlation = features.corr()
    sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.2f')
    plt.title('Matriz de Correla√ß√£o')
    
    # 2. Distribui√ß√£o das features
    plt.subplot(3, 2, 2)
    features.hist(bins=20, alpha=0.7)
    plt.suptitle('Distribui√ß√£o das Features')
    
    # 3. Boxplot
    plt.subplot(3, 2, 3)
    features.boxplot()
    plt.xticks(rotation=45)
    plt.title('Boxplot das Features')
    
    # 4. Scatter das primeiras duas features
    plt.subplot(3, 2, 4)
    if 'species' in data.columns:
        colors = pd.Categorical(data['species']).codes
        scatter = plt.scatter(features.iloc[:, 0], features.iloc[:, 1], c=colors, alpha=0.7)
        plt.colorbar(scatter, label='Esp√©cies')
    else:
        plt.scatter(features.iloc[:, 0], features.iloc[:, 1], alpha=0.7)
    plt.xlabel(features.columns[0])
    plt.ylabel(features.columns[1])
    plt.title('Scatter das 2 primeiras features')
    
    # 5. Distribui√ß√£o por esp√©cie (se dispon√≠vel)
    if 'species' in data.columns:
        plt.subplot(3, 2, 5)
        data['species'].value_counts().plot(kind='bar')
        plt.title('Distribui√ß√£o das Esp√©cies')
        plt.xticks(rotation=45)
        
        # 6. Violinplot de uma feature por esp√©cie
        if len(features.columns) > 0:
            plt.subplot(3, 2, 6)
            sns.violinplot(data=data, x='species', y=features.columns[0])
            plt.title(f'Distribui√ß√£o de {features.columns[0]} por Esp√©cie')
            plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    # Pairplot separado (mais detalhado)
    if 'species' in data.columns and len(features.columns) <= 6:
        plt.figure(figsize=(15, 12))
        data_for_pair = data[list(features.columns) + ['species']]
        sns.pairplot(data_for_pair, hue='species', diag_kind='hist')
        plt.suptitle('Pairplot com Esp√©cies Verdadeiras', y=1.02)
        plt.show()

def perform_pca(data):
    """An√°lise de Componentes Principais"""
    print("\n" + "="*50)
    print("üîç AN√ÅLISE DE COMPONENTES PRINCIPAIS (PCA)")
    print("="*50)
    
    # Preparar dados (apenas features num√©ricas)
    features = data.select_dtypes(include=[np.number])
    
    # Padronizar os dados
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # PCA
    pca = PCA()
    pca_result = pca.fit_transform(features_scaled)
    
    # Vari√¢ncia explicada
    variance_ratio = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(variance_ratio)
    
    print(f"üìä Vari√¢ncia explicada por componente:")
    for i, var in enumerate(variance_ratio):
        print(f"‚Ä¢ PC{i+1}: {var:.3f} ({var*100:.1f}%)")
    
    print(f"\nüìà Vari√¢ncia acumulada:")
    for i, cum_var in enumerate(cumulative_variance):
        print(f"‚Ä¢ PC1-PC{i+1}: {cum_var:.3f} ({cum_var*100:.1f}%)")
    
    # Visualiza√ß√µes
    plt.figure(figsize=(18, 6))
    
    # 1. Vari√¢ncia explicada
    plt.subplot(1, 4, 1)
    plt.bar(range(1, len(variance_ratio) + 1), variance_ratio)
    plt.xlabel('Componentes Principais')
    plt.ylabel('Vari√¢ncia Explicada')
    plt.title('Vari√¢ncia Explicada por Componente')
    
    # 2. Vari√¢ncia acumulada
    plt.subplot(1, 4, 2)
    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')
    plt.axhline(y=0.95, color='r', linestyle='--', label='95%')
    plt.xlabel('N√∫mero de Componentes')
    plt.ylabel('Vari√¢ncia Acumulada')
    plt.title('Vari√¢ncia Acumulada')
    plt.legend()
    
    # 3. Proje√ß√£o 2D
    plt.subplot(1, 4, 3)
    if 'species' in data.columns:
        colors = pd.Categorical(data['species']).codes
        scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=colors, alpha=0.7)
        plt.colorbar(scatter, label='Esp√©cies')
    else:
        plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)
    plt.xlabel(f'PC1 ({variance_ratio[0]*100:.1f}%)')
    plt.ylabel(f'PC2 ({variance_ratio[1]*100:.1f}%)')
    plt.title('Proje√ß√£o PCA 2D')
    
    # 4. Proje√ß√£o 3D (se temos pelo menos 3 componentes)
    if len(variance_ratio) >= 3:
        ax = plt.subplot(1, 4, 4, projection='3d')
        if 'species' in data.columns:
            colors = pd.Categorical(data['species']).codes
            scatter = ax.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], c=colors, alpha=0.7)
        else:
            ax.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], alpha=0.7)
        ax.set_xlabel(f'PC1 ({variance_ratio[0]*100:.1f}%)')
        ax.set_ylabel(f'PC2 ({variance_ratio[1]*100:.1f}%)')
        ax.set_zlabel(f'PC3 ({variance_ratio[2]*100:.1f}%)')
        plt.title('Proje√ß√£o PCA 3D')
    
    plt.tight_layout()
    plt.show()
    
    return pca_result, pca, scaler

def kmeans_clustering(data, features_scaled, max_clusters=10):
    """Clustering K-Means"""
    print("\n" + "="*50)
    print("üéØ CLUSTERING K-MEANS")
    print("="*50)
    
    # M√©todo do cotovelo e silhouette
    inertias = []
    silhouette_scores = []
    K_range = range(2, max_clusters + 1)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(features_scaled)
        inertias.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(features_scaled, kmeans.labels_))
    
    # Encontrar o melhor k pelo silhouette score
    best_k = K_range[np.argmax(silhouette_scores)]
    print(f"üèÜ Melhor n√∫mero de clusters (Silhouette): {best_k}")
    
    # Visualizar m√©tricas
    plt.figure(figsize=(18, 6))
    
    # M√©todo do cotovelo
    plt.subplot(1, 4, 1)
    plt.plot(K_range, inertias, 'bo-')
    plt.xlabel('N√∫mero de Clusters (k)')
    plt.ylabel('In√©rcia')
    plt.title('M√©todo do Cotovelo')
    
    # Silhouette score
    plt.subplot(1, 4, 2)
    plt.plot(K_range, silhouette_scores, 'ro-')
    plt.axvline(x=best_k, color='g', linestyle='--', label=f'Melhor k={best_k}')
    plt.xlabel('N√∫mero de Clusters (k)')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Score')
    plt.legend()
    
    # Clustering final
    kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    clusters = kmeans_final.fit_predict(features_scaled)
    
    # PCA para visualiza√ß√£o
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(features_scaled)
    
    plt.subplot(1, 4, 3)
    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, cmap='viridis', alpha=0.7)
    # Plotar centroides
    centroids_pca = pca.transform(kmeans_final.cluster_centers_)
    plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], 
                c='red', marker='x', s=200, linewidths=3, label='Centroides')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title(f'K-Means Clustering (k={best_k})')
    plt.colorbar(scatter, label='Clusters')
    plt.legend()
    
    # Distribui√ß√£o dos clusters
    plt.subplot(1, 4, 4)
    unique, counts = np.unique(clusters, return_counts=True)
    plt.bar(unique, counts)
    plt.xlabel('Cluster')
    plt.ylabel('N√∫mero de Amostras')
    plt.title('Distribui√ß√£o dos Clusters')
    
    plt.tight_layout()
    plt.show()
    
    # Avaliar qualidade se temos labels verdadeiros
    if 'species' in data.columns:
        true_labels = pd.Categorical(data['species']).codes
        ari = adjusted_rand_score(true_labels, clusters)
        nmi = normalized_mutual_info_score(true_labels, clusters)
        print(f"\nüìä M√©tricas de avalia√ß√£o:")
        print(f"‚Ä¢ Adjusted Rand Index: {ari:.3f}")
        print(f"‚Ä¢ Normalized Mutual Information: {nmi:.3f}")
        print(f"‚Ä¢ Silhouette Score: {silhouette_scores[best_k-2]:.3f}")
        
        # Matriz de confus√£o
        plt.figure(figsize=(8, 6))
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(true_labels, clusters)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('Matriz de Confus√£o: Classes Verdadeiras vs Clusters')
        plt.xlabel('Clusters Preditos')
        plt.ylabel('Classes Verdadeiras')
        plt.show()
    
    return clusters, kmeans_final

def hierarchical_clustering(data, features_scaled):
    """Clustering Hier√°rquico"""
    print("\n" + "="*50)
    print("üå≥ CLUSTERING HIER√ÅRQUICO")
    print("="*50)
    
    # Calcular linkage
    linkage_matrix = linkage(features_scaled, method='ward')
    
    plt.figure(figsize=(18, 12))
    
    # Dendrograma
    plt.subplot(3, 2, 1)
    dendrogram(linkage_matrix, truncate_mode='level', p=5)
    plt.title('Dendrograma (Ward)')
    plt.xlabel('Amostras')
    plt.ylabel('Dist√¢ncia')
    
    # Testar diferentes n√∫meros de clusters
    silhouette_scores = []
    cluster_range = range(2, 8)
    
    for n_clusters in cluster_range:
        hierarchical = AgglomerativeClustering(n_clusters=n_clusters)
        clusters = hierarchical.fit_predict(features_scaled)
        silhouette_scores.append(silhouette_score(features_scaled, clusters))
    
    best_n = cluster_range[np.argmax(silhouette_scores)]
    print(f"üèÜ Melhor n√∫mero de clusters: {best_n}")
    
    # Silhouette scores
    plt.subplot(3, 2, 2)
    plt.plot(cluster_range, silhouette_scores, 'go-')
    plt.axvline(x=best_n, color='r', linestyle='--', label=f'Melhor n={best_n}')
    plt.xlabel('N√∫mero de Clusters')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Score - Hier√°rquico')
    plt.legend()
    
    # Clustering final
    hierarchical_final = AgglomerativeClustering(n_clusters=best_n)
    clusters = hierarchical_final.fit_predict(features_scaled)
    
    # Visualiza√ß√£o 2D (PCA)
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(features_scaled)
    
    plt.subplot(3, 2, 3)
    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, cmap='plasma', alpha=0.7)
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title(f'Clustering Hier√°rquico (n={best_n})')
    plt.colorbar(scatter, label='Clusters')
    
    # Distribui√ß√£o dos clusters
    plt.subplot(3, 2, 4)
    unique, counts = np.unique(clusters, return_counts=True)
    plt.bar(unique, counts)
    plt.xlabel('Cluster')
    plt.ylabel('N√∫mero de Amostras')
    plt.title('Distribui√ß√£o dos Clusters')
    
    # Compara√ß√£o com classes verdadeiras (se dispon√≠vel)
    if 'species' in data.columns:
        true_labels = pd.Categorical(data['species']).codes
        plt.subplot(3, 2, 5)
        scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=true_labels, cmap='Set1', alpha=0.7)
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.title('Classes Verdadeiras')
        plt.colorbar(scatter, label='Esp√©cies')
        
        # M√©tricas
        ari = adjusted_rand_score(true_labels, clusters)
        nmi = normalized_mutual_info_score(true_labels, clusters)
        print(f"\nüìä M√©tricas de avalia√ß√£o:")
        print(f"‚Ä¢ Adjusted Rand Index: {ari:.3f}")
        print(f"‚Ä¢ Normalized Mutual Information: {nmi:.3f}")
        print(f"‚Ä¢ Silhouette Score: {max(silhouette_scores):.3f}")
        
        # Matriz de confus√£o
        plt.subplot(3, 2, 6)
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(true_labels, clusters)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
        plt.title('Matriz de Confus√£o')
        plt.xlabel('Clusters Preditos')
        plt.ylabel('Classes Verdadeiras')
    
    plt.tight_layout()
    plt.show()
    
    return clusters, hierarchical_final

def dbscan_clustering(data, features_scaled):
    """Clustering DBSCAN"""
    print("\n" + "="*50)
    print("üîç CLUSTERING DBSCAN")
    print("="*50)
    
    # Testar diferentes valores de eps
    eps_values = np.arange(0.1, 2.0, 0.1)
    silhouette_scores = []
    n_clusters_list = []
    n_noise_list = []
    
    for eps in eps_values:
        dbscan = DBSCAN(eps=eps, min_samples=5)
        clusters = dbscan.fit_predict(features_scaled)
        
        n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
        n_noise = list(clusters).count(-1)
        
        n_clusters_list.append(n_clusters)
        n_noise_list.append(n_noise)
        
        if n_clusters > 1:
            # Calcular silhouette score apenas para pontos n√£o-noise
            non_noise_mask = clusters != -1
            if np.sum(non_noise_mask) > 1:
                score = silhouette_score(features_scaled[non_noise_mask], clusters[non_noise_mask])
                silhouette_scores.append(score)
            else:
                silhouette_scores.append(-1)
        else:
            silhouette_scores.append(-1)
    
    # Encontrar melhor eps
    valid_scores = [s for s in silhouette_scores if s > -1]
    if valid_scores:
        best_idx = np.argmax(silhouette_scores)
        best_eps = eps_values[best_idx]
        print(f"üèÜ Melhor eps: {best_eps:.2f}")
    else:
        best_eps = 0.5
        print(f"‚ö†Ô∏è Usando eps padr√£o: {best_eps}")
    
    # Visualizar m√©tricas
    plt.figure(figsize=(18, 12))
    
    # N√∫mero de clusters vs eps
    plt.subplot(3, 3, 1)
    plt.plot(eps_values, n_clusters_list, 'b-')
    plt.axvline(x=best_eps, color='r', linestyle='--', label=f'Melhor eps={best_eps:.2f}')
    plt.xlabel('eps')
    plt.ylabel('N√∫mero de Clusters')
    plt.title('N√∫mero de Clusters vs eps')
    plt.legend()
    
    # N√∫mero de pontos noise vs eps
    plt.subplot(3, 3, 2)
    plt.plot(eps_values, n_noise_list, 'g-')
    plt.axvline(x=best_eps, color='r', linestyle='--', label=f'Melhor eps={best_eps:.2f}')
    plt.xlabel('eps')
    plt.ylabel('Pontos Noise')
    plt.title('Pontos Noise vs eps')
    plt.legend()
    
    # Silhouette score vs eps
    plt.subplot(3, 3, 3)
    plt.plot(eps_values, silhouette_scores, 'r-')
    plt.axvline(x=best_eps, color='r', linestyle='--', label=f'Melhor eps={best_eps:.2f}')
    plt.xlabel('eps')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Score vs eps')
    plt.legend()
    
    # Clustering final
    dbscan_final = DBSCAN(eps=best_eps, min_samples=5)
    clusters = dbscan_final.fit_predict(features_scaled)
    
    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
    n_noise = list(clusters).count(-1)
    
    print(f"\nüìä Resultados finais:")
    print(f"‚Ä¢ N√∫mero de clusters: {n_clusters}")
    print(f"‚Ä¢ Pontos noise: {n_noise}")
    print(f"‚Ä¢ Porcentagem noise: {n_noise/len(clusters)*100:.1f}%")
    
    # Visualiza√ß√£o 2D
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(features_scaled)
    
    plt.subplot(3, 3, 4)
    unique_labels = set(clusters)
    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
    
    for k, col in zip(unique_labels, colors):
        if k == -1:
            # Pontos noise em preto
            col = 'black'
            marker = 'x'
            label = 'Noise'
        else:
            marker = 'o'
            label = f'Cluster {k}'
        
        class_member_mask = (clusters == k)
        xy = pca_result[class_member_mask]
        plt.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, alpha=0.7, s=50, label=label)
    
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title(f'DBSCAN (eps={best_eps:.2f})')
    plt.legend()
    
    # Distribui√ß√£o dos clusters (excluindo noise)
    plt.subplot(3, 3, 5)
    non_noise_clusters = clusters[clusters != -1]
    if len(non_noise_clusters) > 0:
        unique, counts = np.unique(non_noise_clusters, return_counts=True)
        plt.bar(unique, counts)
        plt.xlabel('Cluster')
        plt.ylabel('N√∫mero de Amostras')
        plt.title('Distribui√ß√£o dos Clusters (sem noise)')
    
    # Compara√ß√£o com classes verdadeiras
    if 'species' in data.columns:
        true_labels = pd.Categorical(data['species']).codes
        plt.subplot(3, 3, 6)
        scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=true_labels, cmap='Set1', alpha=0.7)
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.title('Classes Verdadeiras')
        plt.colorbar(scatter, label='Esp√©cies')
        
        # M√©tricas (excluindo pontos noise)
        non_noise_mask = clusters != -1
        if np.sum(non_noise_mask) > 0:
            ari = adjusted_rand_score(true_labels[non_noise_mask], clusters[non_noise_mask])
            nmi = normalized_mutual_info_score(true_labels[non_noise_mask], clusters[non_noise_mask])
            
            if len(set(clusters[non_noise_mask])) > 1:
                sil_score = silhouette_score(features_scaled[non_noise_mask], clusters[non_noise_mask])
            else:
                sil_score = -1
            
            print(f"\nüìä M√©tricas de avalia√ß√£o (sem noise):")
            print(f"‚Ä¢ Adjusted Rand Index: {ari:.3f}")
            print(f"‚Ä¢ Normalized Mutual Information: {nmi:.3f}")
            print(f"‚Ä¢ Silhouette Score: {sil_score:.3f}")
        
        # Matriz de confus√£o (sem noise)
        if np.sum(non_noise_mask) > 0:
            plt.subplot(3, 3, 7)
            from sklearn.metrics import confusion_matrix
            cm = confusion_matrix(true_labels[non_noise_mask], clusters[non_noise_mask])
            sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges')
            plt.title('Matriz de Confus√£o (sem noise)')
            plt.xlabel('Clusters Preditos')
            plt.ylabel('Classes Verdadeiras')
    
    plt.tight_layout()
    plt.show()
    
    return clusters, dbscan_final

def compare_algorithms(data, features_scaled):
    """Comparar todos os algoritmos de clustering"""
    print("\n" + "="*50)
    print("üèÜ COMPARA√á√ÉO DE ALGORITMOS")
    print("="*50)
    
    # Executar todos os algoritmos
    algorithms = {}
    
    # K-Means
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    kmeans_clusters = kmeans.fit_predict(features_scaled)
    algorithms['K-Means'] = kmeans_clusters
    
    # Hier√°rquico
    hierarchical = AgglomerativeClustering(n_clusters=3)
    hierarchical_clusters = hierarchical.fit_predict(features_scaled)
    algorithms['Hier√°rquico'] = hierarchical_clusters
    
    # DBSCAN
    dbscan = DBSCAN(eps=0.5, min_samples=5)
    dbscan_clusters = dbscan.fit_predict(features_scaled)
    algorithms['DBSCAN'] = dbscan_clusters
    
    # Visualiza√ß√µes comparativas
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(features_scaled)
    
    plt.figure(figsize=(20, 12))
    
    for i, (name, clusters) in enumerate(algorithms.items(), 1):
        plt.subplot(2, 4, i)
        if name == 'DBSCAN':
            # Tratamento especial para DBSCAN (pontos noise)
            unique_labels = set(clusters)
            colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
            
            for k, col in zip(unique_labels, colors):
                if k == -1:
                    col = 'black'
                    marker = 'x'
                else:
                    marker = 'o'
                
                class_member_mask = (clusters == k)
                xy = pca_result[class_member_mask]
                plt.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, alpha=0.7)
        else:
            scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, alpha=0.7)
            plt.colorbar(scatter, label='Clusters')
        
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.title(name)
    
    # Classes verdadeiras
    if 'species' in data.columns:
        true_labels = pd.Categorical(data['species']).codes
        plt.subplot(2, 4, 4)
        scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=true_labels, cmap='Set1', alpha=0.7)
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.title('Classes Verdadeiras')
        plt.colorbar(scatter, label='Esp√©cies')
        
        # Gr√°fico de barras com m√©tricas
        plt.subplot(2, 4, 5)
        metrics_data = []
        algorithm_names = []
        
        for name, clusters in algorithms.items():
            if name == 'DBSCAN':
                # Para DBSCAN, considerar apenas pontos n√£o-noise
                non_noise_mask = clusters != -1
                if np.sum(non_noise_mask) > 0:
                    ari = adjusted_rand_score(true_labels[non_noise_mask], clusters[non_noise_mask])
                    nmi = normalized_mutual_info_score(true_labels[non_noise_mask], clusters[non_noise_mask])
                    if len(set(clusters[non_noise_mask])) > 1:
                        sil = silhouette_score(features_scaled[non_noise_mask], clusters[non_noise_mask])
                    else:
                        sil = -1
                else:
                    ari, nmi, sil = -1, -1, -1
            else:
                ari = adjusted_rand_score(true_labels, clusters)
                nmi = normalized_mutual_info_score(true_labels, clusters)
                if len(set(clusters)) > 1:
                    sil = silhouette_score(features_scaled, clusters)
                else:
                    sil = -1
            
            metrics_data.append([ari, nmi, sil])
            algorithm_names.append(name)
        
        metrics_df = pd.DataFrame(metrics_data, columns=['ARI', 'NMI', 'Silhouette'], index=algorithm_names)
        metrics_df.plot(kind='bar', ax=plt.gca())
        plt.title('Compara√ß√£o de M√©tricas')
        plt.ylabel('Score')
        plt.xticks(rotation=45)
        plt.legend()
        
        # Tabela de m√©tricas
        print(f"\nüìä Tabela de M√©tricas:")
        print(f"{'Algoritmo':<15} {'ARI':<6} {'NMI':<6} {'Silhouette':<10}")
        print("-" * 45)
        
        for name, (ari, nmi, sil) in zip(algorithm_names, metrics_data):
            print(f"{name:<15} {ari:<6.3f} {nmi:<6.3f} {sil:<10.3f}")
    
    plt.tight_layout()
    plt.show()

def analyze_clusters_characteristics(data, clusters, algorithm_name):
    """Analisa as caracter√≠sticas dos clusters encontrados"""
    print(f"\nüìä AN√ÅLISE DE CARACTER√çSTICAS - {algorithm_name}")
    print("="*50)
    
    # Adicionar clusters ao dataframe
    data_with_clusters = data.copy()
    data_with_clusters['cluster'] = clusters
    
    # Features num√©ricas
    numeric_features = data.select_dtypes(include=[np.number])
    
    # Estat√≠sticas por cluster
    print("\nüìà Estat√≠sticas por cluster:")
    cluster_stats = data_with_clusters.groupby('cluster')[numeric_features.columns].mean()
    print(cluster_stats)
    
    # Visualiza√ß√£o das caracter√≠sticas
    if len(numeric_features.columns) <= 6:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        for i, feature in enumerate(numeric_features.columns):
            if i < 6:
                data_with_clusters.boxplot(column=feature, by='cluster', ax=axes[i])
                axes[i].set_title(f'{feature} por Cluster')
                axes[i].set_xlabel('Cluster')
        
        plt.tight_layout()
        plt.show()

def main():
    """Fun√ß√£o principal"""
    print("üêß AN√ÅLISE DE CLUSTERING - DATASET PENGUIN")
    print("="*60)
    
    # 1. Carregar dados
    data = load_penguin_dataset()
    if data is None:
        print("‚ùå N√£o foi poss√≠vel carregar o dataset!")
        return
    
    print(f"‚úÖ Dataset carregado com sucesso!")
    print(f"üìä Shape: {data.shape}")
    
    # 2. Limpar e preparar dados
    data_clean, label_encoders = clean_and_prepare_data(data)
    
    # 3. An√°lise explorat√≥ria
    explore_data(data_clean)
    
    # 4. Visualiza√ß√µes
    visualize_data(data_clean)
    
    # 5. Preparar dados para clustering
    features = data_clean.select_dtypes(include=[np.number])
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # 6. PCA
    pca_result, pca_model, scaler_model = perform_pca(data_clean)
    
    # 7. K-Means
    kmeans_clusters, kmeans_model = kmeans_clustering(data_clean, features_scaled)
    analyze_clusters_characteristics(data_clean, kmeans_clusters, "K-MEANS")
    
    # 8. Clustering Hier√°rquico
    hierarchical_clusters, hierarchical_model = hierarchical_clustering(data_clean, features_scaled)
    analyze_clusters_characteristics(data_clean, hierarchical_clusters, "HIER√ÅRQUICO")
    
    # 9. DBSCAN
    dbscan_clusters, dbscan_model = dbscan_clustering(data_clean, features_scaled)
    # Para DBSCAN, analisar apenas pontos n√£o-noise
    non_noise_mask = dbscan_clusters != -1
    if np.sum(non_noise_mask) > 0:
        analyze_clusters_characteristics(data_clean[non_noise_mask], dbscan_clusters[non_noise_mask], "DBSCAN (sem noise)")
    
    # 10. Compara√ß√£o
    compare_algorithms(data_clean, features_scaled)
    
    print("\n" + "="*60)
    print("üéâ AN√ÅLISE COMPLETA!")
    print("="*60)
    print("\nüìã Resumo dos resultados:")
    print("‚Ä¢ Dados limpos e preparados")
    print("‚Ä¢ PCA: Redu√ß√£o de dimensionalidade realizada")
    print("‚Ä¢ K-Means: Clustering particional executado")
    print("‚Ä¢ Hier√°rquico: Clustering hier√°rquico executado")
    print("‚Ä¢ DBSCAN: Clustering baseado em densidade executado")
    print("‚Ä¢ Compara√ß√£o: Todos os algoritmos foram comparados")
    print("‚Ä¢ Caracter√≠sticas: An√°lise detalhada dos clusters")
    
    if 'species' in data_clean.columns:
        print("\nüí° Dicas para interpreta√ß√£o:")
        print("‚Ä¢ ARI pr√≥ximo de 1: clustering muito similar √†s classes verdadeiras")
        print("‚Ä¢ NMI pr√≥ximo de 1: alta informa√ß√£o m√∫tua entre clusters e classes")
        print("‚Ä¢ Silhouette pr√≥ximo de 1: clusters bem definidos e separados")
        print("‚Ä¢ DBSCAN pode identificar outliers como pontos de noise")

if __name__ == "__main__":
    main()
