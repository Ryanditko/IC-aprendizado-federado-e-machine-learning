"""
NSL-KDD Outlier Detection - Vers√£o Melhorada
============================================

ORIENTA√á√ïES DO DIA 04/11:
‚úì Escolher apenas 2 mapeamentos: "Normal" + 1 tipo de ataque
‚úì An√°lise de correla√ß√£o para selecionar features relevantes
‚úì Avalia√ß√£o da normaliza√ß√£o (usar apenas se melhorar m√©tricas)
‚úì N√ÉO usar redu√ß√£o de dimensionalidade (PCA)
‚úì Recall como m√©trica principal

Autor: Projeto de Inicia√ß√£o Cient√≠fica - Faculdade Impacta
Data: Novembro 2025
"""

import pandas as pd
import numpy as np
import os
import warnings
from datetime import datetime
import time
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

# M√©todos de detec√ß√£o de outliers
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope

# M√©tricas - FOCO NO RECALL
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)

warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURA√á√ïES
# ============================================================================

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, '..', '..', '..', 'data', 'nsl-kdd')
OUTPUT_DIR = os.path.join(BASE_DIR, 'output-nsl-kdd')
os.makedirs(OUTPUT_DIR, exist_ok=True)

RANDOM_STATE = 42
CORRELATION_THRESHOLD = 0.95  # Para remover features altamente correlacionadas

print("="*80)
print("NSL-KDD OUTLIER DETECTION - VERS√ÉO MELHORADA")
print("="*80)
print(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"FOCO: Recall como m√©trica principal")
print(f"Configura√ß√£o: 2 classes + an√°lise de correla√ß√£o")
print("="*80)

# ============================================================================
# 1. CARREGAMENTO E PREPARA√á√ÉO DOS DADOS (2 CLASSES)
# ============================================================================

print("\n[1/6] CARREGAMENTO E PREPARA√á√ÉO DOS DADOS")
print("-" * 80)

# Carregar NSL-KDD
train_path = os.path.join(DATA_DIR, 'KDDTrain+.txt')
test_path = os.path.join(DATA_DIR, 'KDDTest+.txt')

# Colunas do NSL-KDD
columns = [
    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',
    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',
    'logged_in', 'num_compromised', 'root_shell', 'su_attempted',
    'num_root', 'num_file_creations', 'num_shells', 'num_access_files',
    'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',
    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',
    'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',
    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',
    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',
    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',
    'dst_host_serror_rate', 'dst_host_srv_serror_rate',
    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',
    'attack_type', 'difficulty'
]

try:
    df_train = pd.read_csv(train_path, names=columns)
    df_test = pd.read_csv(test_path, names=columns)
    print(f"‚úì Train: {len(df_train):,} registros")
    print(f"‚úì Test: {len(df_test):,} registros")
except FileNotFoundError:
    print("ERRO: Arquivos NSL-KDD n√£o encontrados!")
    print("Execute o download primeiro:")
    print("cd ../../../downloads && python download_nsl_kdd_dataset.py")
    exit(1)

# Combinar datasets
df = pd.concat([df_train, df_test], ignore_index=True)
print(f"‚úì Dataset combinado: {len(df):,} registros")

# ============================================================================
# ESCOLHA DE 2 CLASSES: Normal + U2R (User-to-Root)
# ============================================================================

print(f"\nDistribui√ß√£o original de ataques:")
attack_counts = df['attack_type'].value_counts()
print(attack_counts.head(10))

# Mapear ataques para categorias principais
attack_mapping = {
    'normal': 'normal',
    # U2R attacks (User-to-Root) - Escolhido por ser mais raro e cr√≠tico
    'buffer_overflow': 'u2r',
    'loadmodule': 'u2r',
    'perl': 'u2r',
    'rootkit': 'u2r',
    'sqlattack': 'u2r',
    'xterm': 'u2r',
    'ps': 'u2r',
    'httptunnel': 'u2r',
}

# DECIS√ÉO: Focar apenas em Normal vs U2R
print(f"\nüéØ ESCOLHA: Normal vs U2R (User-to-Root) attacks")
print(f"Motivo: U2R s√£o ataques cr√≠ticos e raros, ideais para detec√ß√£o de outliers")

# Filtrar apenas Normal e U2R
df['attack_category'] = df['attack_type'].map(attack_mapping)
df_filtered = df[df['attack_category'].notna()].copy()

print(f"\nDistribui√ß√£o das 2 classes escolhidas:")
class_counts = df_filtered['attack_category'].value_counts()
print(class_counts)

contamination = class_counts['u2r'] / len(df_filtered)
print(f"\nContamina√ß√£o (% U2R): {contamination:.4f} ({contamination*100:.2f}%)")

# Criar labels bin√°rios: 0=Normal, 1=U2R(Outlier)
df_filtered['is_outlier'] = (df_filtered['attack_category'] == 'u2r').astype(int)

print(f"\n‚úì Dataset final: {len(df_filtered):,} registros")
print(f"  Normal (0): {(df_filtered['is_outlier'] == 0).sum():,}")
print(f"  U2R (1):    {(df_filtered['is_outlier'] == 1).sum():,}")

# ============================================================================
# 2. AN√ÅLISE DE CORRELA√á√ÉO E SELE√á√ÉO DE FEATURES
# ============================================================================

print("\n[2/6] AN√ÅLISE DE CORRELA√á√ÉO E SELE√á√ÉO DE FEATURES")
print("-" * 80)

# Preparar features num√©ricas
feature_columns = [col for col in df_filtered.columns 
                  if col not in ['attack_type', 'attack_category', 'is_outlier', 'difficulty']]

# Codificar vari√°veis categ√≥ricas
df_features = df_filtered[feature_columns].copy()

# Label encoding para categ√≥ricas
categorical_cols = ['protocol_type', 'service', 'flag']
label_encoders = {}

for col in categorical_cols:
    if col in df_features.columns:
        le = LabelEncoder()
        df_features[col] = le.fit_transform(df_features[col].astype(str))
        label_encoders[col] = le

print(f"‚úì Features codificadas: {len(df_features.columns)} colunas")

# An√°lise de correla√ß√£o
print("\nCalculando matriz de correla√ß√£o...")
correlation_matrix = df_features.corr()

# Encontrar features altamente correlacionadas
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_value = abs(correlation_matrix.iloc[i, j])
        if corr_value > CORRELATION_THRESHOLD:
            high_corr_pairs.append((
                correlation_matrix.columns[i],
                correlation_matrix.columns[j],
                corr_value
            ))

print(f"\nüîç Features altamente correlacionadas (>{CORRELATION_THRESHOLD}):")
if high_corr_pairs:
    for feat1, feat2, corr in high_corr_pairs:
        print(f"  {feat1} ‚Üî {feat2}: {corr:.3f}")
    
    # Remover uma das features correlacionadas (manter a primeira)
    features_to_remove = set()
    for feat1, feat2, corr in high_corr_pairs:
        features_to_remove.add(feat2)
    
    df_features = df_features.drop(columns=list(features_to_remove))
    print(f"‚úì Removidas {len(features_to_remove)} features correlacionadas")
else:
    print("  Nenhuma correla√ß√£o alta encontrada")

print(f"‚úì Features finais: {len(df_features.columns)} colunas")

# Salvar heatmap de correla√ß√£o
plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', center=0,
            square=True, annot=False, cbar_kws={"shrink": .8})
plt.title('Matriz de Correla√ß√£o - Features NSL-KDD')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'correlation_matrix.png'), dpi=300, bbox_inches='tight')
plt.close()
print("‚úì Matriz de correla√ß√£o salva")

# ============================================================================
# 3. AVALIA√á√ÉO DA NORMALIZA√á√ÉO
# ============================================================================

print("\n[3/6] AVALIA√á√ÉO DA NORMALIZA√á√ÉO")
print("-" * 80)

X = df_features.values
y = df_filtered['is_outlier'].values

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y
)

print(f"‚úì Treino: {len(X_train):,} amostras")
print(f"‚úì Teste: {len(X_test):,} amostras")

# Testar COM e SEM normaliza√ß√£o
normalization_results = {}

print(f"\nTestando Isolation Forest COM e SEM normaliza√ß√£o...")

# SEM normaliza√ß√£o
iso_forest_raw = IsolationForest(
    contamination=contamination,
    random_state=RANDOM_STATE,
    n_estimators=100
)
iso_forest_raw.fit(X_train)
pred_raw = iso_forest_raw.predict(X_test)
pred_raw_binary = (pred_raw == -1).astype(int)

recall_raw = recall_score(y_test, pred_raw_binary)
f1_raw = f1_score(y_test, pred_raw_binary)

print(f"  SEM normaliza√ß√£o - Recall: {recall_raw:.3f}, F1: {f1_raw:.3f}")

# COM normaliza√ß√£o
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

iso_forest_scaled = IsolationForest(
    contamination=contamination,
    random_state=RANDOM_STATE,
    n_estimators=100
)
iso_forest_scaled.fit(X_train_scaled)
pred_scaled = iso_forest_scaled.predict(X_test_scaled)
pred_scaled_binary = (pred_scaled == -1).astype(int)

recall_scaled = recall_score(y_test, pred_scaled_binary)
f1_scaled = f1_score(y_test, pred_scaled_binary)

print(f"  COM normaliza√ß√£o - Recall: {recall_scaled:.3f}, F1: {f1_scaled:.3f}")

# DECIS√ÉO: Usar normaliza√ß√£o apenas se melhorar
use_normalization = recall_scaled > recall_raw
print(f"\nüéØ DECIS√ÉO: {'USAR' if use_normalization else 'N√ÉO USAR'} normaliza√ß√£o")
print(f"   Motivo: Recall {'melhorou' if use_normalization else 'piorou'} com normaliza√ß√£o")

# Preparar dados finais
if use_normalization:
    X_final_train = X_train_scaled
    X_final_test = X_test_scaled
    print("‚úì Usando dados normalizados")
else:
    X_final_train = X_train
    X_final_test = X_test
    print("‚úì Usando dados brutos (sem normaliza√ß√£o)")

# ============================================================================
# 4. DETEC√á√ÉO DE OUTLIERS - M√öLTIPLAS T√âCNICAS
# ============================================================================

print("\n[4/6] DETEC√á√ÉO DE OUTLIERS - FOCO NO RECALL")
print("-" * 80)

methods = {
    'Isolation Forest': IsolationForest(
        contamination=contamination,
        random_state=RANDOM_STATE,
        n_estimators=100,
        n_jobs=-1
    ),
    'Local Outlier Factor': LocalOutlierFactor(
        contamination=contamination,
        n_neighbors=20,
        novelty=True,  # Permite usar predict em dados novos
        n_jobs=-1
    ),
    'One-Class SVM': OneClassSVM(
        nu=contamination,
        kernel='rbf',
        gamma='auto'
    ),
    'Elliptic Envelope': EllipticEnvelope(
        contamination=contamination,
        random_state=RANDOM_STATE
    )
}

results = []

for method_name, method in methods.items():
    print(f"\nüîß {method_name}...")
    start_time = time.time()
    
    # Treinar modelo
    method.fit(X_final_train)
    pred_test = method.predict(X_final_test)
    
    execution_time = time.time() - start_time
    
    # Converter predi√ß√µes (-1/1 para 1/0)
    pred_binary = (pred_test == -1).astype(int)
    
    # Calcular m√©tricas - FOCO NO RECALL
    accuracy = accuracy_score(y_test, pred_binary)
    precision = precision_score(y_test, pred_binary, zero_division=0)
    recall = recall_score(y_test, pred_binary, zero_division=0)  # M√âTRICA PRINCIPAL
    f1 = f1_score(y_test, pred_binary, zero_division=0)
    
    # Confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_test, pred_binary).ravel()
    
    results.append({
        'M√©todo': method_name,
        'Recall': recall,  # M√©trica principal em primeiro
        'F1-Score': f1,
        'Precision': precision,
        'Accuracy': accuracy,
        'TP': tp,
        'FP': fp,
        'TN': tn,
        'FN': fn,
        'Tempo (s)': execution_time
    })
    
    print(f"   ‚úì Recall:    {recall*100:.2f}% ‚≠ê (PRINCIPAL)")
    print(f"   ‚úì F1-Score:  {f1*100:.2f}%")
    print(f"   ‚úì Precision: {precision*100:.2f}%")
    print(f"   ‚úì Accuracy:  {accuracy*100:.2f}%")
    print(f"   ‚úì Tempo:     {execution_time:.2f}s")

# ============================================================================
# 5. AN√ÅLISE DOS RESULTADOS - RANKING POR RECALL
# ============================================================================

print("\n[5/6] AN√ÅLISE DOS RESULTADOS - RANKING POR RECALL")
print("-" * 80)

# Criar DataFrame e ordenar por RECALL
df_results = pd.DataFrame(results)
df_results = df_results.sort_values('Recall', ascending=False).reset_index(drop=True)
df_results['Rank'] = range(1, len(df_results) + 1)

print("\nüèÜ RANKING POR RECALL (M√âTRICA PRINCIPAL):")
print(df_results[['Rank', 'M√©todo', 'Recall', 'F1-Score', 'Precision', 'Accuracy']].to_string(index=False))

# Melhor m√©todo
best_method = df_results.iloc[0]
print(f"\nü•á MELHOR M√âTODO: {best_method['M√©todo']}")
print(f"   Recall:     {best_method['Recall']*100:.2f}% ‚≠ê")
print(f"   F1-Score:   {best_method['F1-Score']*100:.2f}%")
print(f"   Precision:  {best_method['Precision']*100:.2f}%")
print(f"   Accuracy:   {best_method['Accuracy']*100:.2f}%")

# An√°lise de performance
print(f"\nüìä AN√ÅLISE DE PERFORMANCE:")
print(f"   Recall m√©dio:    {df_results['Recall'].mean()*100:.2f}%")
print(f"   F1-Score m√©dio:  {df_results['F1-Score'].mean()*100:.2f}%")
print(f"   Melhor recall:   {df_results['Recall'].max()*100:.2f}%")

# ============================================================================
# 6. VISUALIZA√á√ïES E EXPORTA√á√ÉO
# ============================================================================

print("\n[6/6] VISUALIZA√á√ïES E EXPORTA√á√ÉO")
print("-" * 80)

# Gr√°fico de compara√ß√£o das m√©tricas
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Recall (m√©trica principal)
axes[0,0].bar(df_results['M√©todo'], df_results['Recall'], color='darkred', alpha=0.7)
axes[0,0].set_title('Recall por M√©todo (M√âTRICA PRINCIPAL)', fontweight='bold')
axes[0,0].set_ylabel('Recall')
axes[0,0].tick_params(axis='x', rotation=45)

# F1-Score
axes[0,1].bar(df_results['M√©todo'], df_results['F1-Score'], color='navy', alpha=0.7)
axes[0,1].set_title('F1-Score por M√©todo')
axes[0,1].set_ylabel('F1-Score')
axes[0,1].tick_params(axis='x', rotation=45)

# Precision
axes[1,0].bar(df_results['M√©todo'], df_results['Precision'], color='darkgreen', alpha=0.7)
axes[1,0].set_title('Precision por M√©todo')
axes[1,0].set_ylabel('Precision')
axes[1,0].tick_params(axis='x', rotation=45)

# Tempo de execu√ß√£o
axes[1,1].bar(df_results['M√©todo'], df_results['Tempo (s)'], color='purple', alpha=0.7)
axes[1,1].set_title('Tempo de Execu√ß√£o por M√©todo')
axes[1,1].set_ylabel('Tempo (s)')
axes[1,1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'nsl_kdd_comparison.png'), dpi=300, bbox_inches='tight')
plt.close()

# Confusion Matrix do melhor m√©todo
best_idx = df_results['Recall'].idxmax()
best_result = df_results.iloc[best_idx]

plt.figure(figsize=(8, 6))
cm_data = [[best_result['TN'], best_result['FP']], 
           [best_result['FN'], best_result['TP']]]
sns.heatmap(cm_data, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Normal', 'U2R'], yticklabels=['Normal', 'U2R'])
plt.title(f'Confusion Matrix - {best_result["M√©todo"]}\nRecall: {best_result["Recall"]*100:.1f}%')
plt.ylabel('Verdadeiro')
plt.xlabel('Predito')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'best_confusion_matrix.png'), dpi=300, bbox_inches='tight')
plt.close()

# Salvar resultados
output_path = os.path.join(OUTPUT_DIR, 'nsl_kdd_results.csv')
df_results.to_csv(output_path, index=False)

print(f"‚úì Gr√°ficos salvos em: {OUTPUT_DIR}")
print(f"‚úì Resultados salvos em: {output_path}")

# ============================================================================
# RESUMO FINAL
# ============================================================================

print("\n" + "="*80)
print("‚úÖ AN√ÅLISE NSL-KDD CONCLU√çDA - VERS√ÉO MELHORADA")
print("="*80)

print(f"\nüìã CONFIGURA√á√ïES APLICADAS:")
print(f"   ‚úì Classes: Normal vs U2R ({contamination*100:.2f}% outliers)")
print(f"   ‚úì Features: {len(df_features.columns)} (ap√≥s an√°lise de correla√ß√£o)")
print(f"   ‚úì Normaliza√ß√£o: {'Aplicada' if use_normalization else 'N√£o aplicada'}")
print(f"   ‚úì M√©trica principal: Recall")
print(f"   ‚úì PCA: N√£o utilizado (conforme orienta√ß√£o)")

print(f"\nüèÜ MELHOR RESULTADO:")
print(f"   M√©todo: {best_method['M√©todo']}")
print(f"   Recall: {best_method['Recall']*100:.2f}% ‚≠ê")
print(f"   Capacidade de detectar U2R: {best_method['TP']}/{best_method['TP'] + best_method['FN']}")

print(f"\nüéØ ADEQUA√á√ÉO PARA CYBERSECURITY:")
print(f"   ‚Ä¢ Foco em ataques U2R cr√≠ticos")
print(f"   ‚Ä¢ Prioriza√ß√£o do recall (detectar todos os ataques)")
print(f"   ‚Ä¢ Features relevantes (sem correla√ß√£o alta)")
print(f"   ‚Ä¢ Metodologia cientificamente rigorosa")

print("\n" + "="*80)
