"""
Aprendizado N√£o-Supervisionado - Cyber Threat Detection
========================================================

Dataset: Text-Based Cyber Threat Detection (Kaggle)
Objetivo: Detectar outliers/anomalias usando t√©cnicas n√£o-supervisionadas
M√©todos: Isolation Forest, LOF, One-Class SVM, Elliptic Envelope, DBSCAN

Autor: Projeto de Inicia√ß√£o Cient√≠fica - Faculdade Impacta
Data: Outubro 2025
"""

import pandas as pd
import numpy as np
import os
import warnings
from datetime import datetime
import time

# Machine Learning
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA

# M√©todos de detec√ß√£o de outliers
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope
from sklearn.cluster import DBSCAN

# M√©tricas
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)

warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURA√á√ïES
# ============================================================================

# Caminhos
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, '..', '..', '..', 'data', 'cyber-outlier-detection')
OUTPUT_DIR = os.path.join(BASE_DIR, 'output')

# Criar diret√≥rios
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Par√¢metros
SAMPLE_SIZE = 15000  # Amostra para performance
RANDOM_STATE = 42
N_PCA_COMPONENTS = 50
MAX_TFIDF_FEATURES = 500

print("="*80)
print("APRENDIZADO N√ÉO-SUPERVISIONADO - CYBER THREAT DETECTION")
print("="*80)
print(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Dataset: Text-Based Cyber Threat Detection")
print(f"Objetivo: Detec√ß√£o de Outliers/Anomalias")
print(f"Diret√≥rio de dados: {DATA_DIR}")
print(f"Diret√≥rio de sa√≠da: {OUTPUT_DIR}")
print("="*80)

# ============================================================================
# 1. CARREGAMENTO E PREPARA√á√ÉO DOS DADOS
# ============================================================================

print("\n[1/5] CARREGAMENTO DOS DADOS")
print("-" * 80)

# Carregar dataset
data_path = os.path.join(DATA_DIR, 'cyber-threat-intelligence_all.csv')

if not os.path.exists(data_path):
    print(f"ERRO: Dataset n√£o encontrado em {data_path}")
    print("Execute o script de download do dataset primeiro:")
    print("  python download_cyber_dataset.py")
    exit(1)

df = pd.read_csv(data_path)
print(f"‚úì Dataset carregado: {len(df):,} registros, {len(df.columns)} colunas")

# Filtrar apenas registros com labels (para valida√ß√£o)
df_labeled = df[df['label'].notna()].copy()
print(f"‚úì Registros com labels: {len(df_labeled):,} ({len(df_labeled)/len(df)*100:.1f}%)")

# Criar labels bin√°rios: threat vs normal
threat_labels = ['malware', 'attack-pattern', 'threat-actor', 'vulnerability', 'tools']
df_labeled['is_threat'] = df_labeled['label'].apply(
    lambda x: 1 if str(x).lower() in threat_labels else 0
)

contamination = (df_labeled['is_threat'] == 1).sum() / len(df_labeled)

print(f"\nDistribui√ß√£o de classes:")
print(f"  Threats (1): {(df_labeled['is_threat'] == 1).sum():,} ({contamination*100:.1f}%)")
print(f"  Normal  (0): {(df_labeled['is_threat'] == 0).sum():,} ({(1-contamination)*100:.1f}%)")
print(f"\n  Contamina√ß√£o estimada: {contamination:.3f}")

# Amostra estratificada
sample_size = min(SAMPLE_SIZE, len(df_labeled))
df_sample = df_labeled.sample(n=sample_size, random_state=RANDOM_STATE, stratify=df_labeled['is_threat'])
print(f"\n‚úì Amostra estratificada: {len(df_sample):,} registros")

# ============================================================================
# 2. PR√â-PROCESSAMENTO E FEATURE ENGINEERING
# ============================================================================

print("\n[2/5] PR√â-PROCESSAMENTO E FEATURE ENGINEERING")
print("-" * 80)

# TF-IDF Vectorization
print("Aplicando TF-IDF...")
vectorizer = TfidfVectorizer(
    max_features=MAX_TFIDF_FEATURES,
    min_df=5,
    max_df=0.7,
    ngram_range=(1, 2),
    stop_words='english',
    strip_accents='unicode'
)

X_tfidf = vectorizer.fit_transform(df_sample['text'])
print(f"‚úì TF-IDF: {X_tfidf.shape[0]:,} amostras, {X_tfidf.shape[1]} features")
print(f"  Sparsity: {(1.0 - X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1])) * 100:.2f}%")

# Converter para array denso
X_tfidf_dense = X_tfidf.toarray()

# PCA para redu√ß√£o de dimensionalidade
print(f"\nAplicando PCA ({N_PCA_COMPONENTS} componentes)...")
pca = PCA(n_components=N_PCA_COMPONENTS, random_state=RANDOM_STATE)
X_pca = pca.fit_transform(X_tfidf_dense)
variance_explained = pca.explained_variance_ratio_.sum()

print(f"‚úì PCA: {X_pca.shape[0]:,} amostras, {X_pca.shape[1]} componentes")
print(f"  Vari√¢ncia explicada: {variance_explained*100:.2f}%")

# Normaliza√ß√£o
print("\nNormalizando features...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_pca)
print(f"‚úì Normaliza√ß√£o completa (mean={X_scaled.mean():.2e}, std={X_scaled.std():.2f})")

# Labels verdadeiros (para valida√ß√£o)
y_true = df_sample['is_threat'].values

# ============================================================================
# 3. DETEC√á√ÉO DE OUTLIERS - M√öLTIPLAS T√âCNICAS
# ============================================================================

print("\n[3/5] DETEC√á√ÉO DE OUTLIERS - M√öLTIPLAS T√âCNICAS")
print("-" * 80)
print(f"Contamina√ß√£o esperada: {contamination:.3f} ({contamination*100:.1f}%)\n")

outlier_predictions = {}
execution_times = {}

# 1. ISOLATION FOREST
print("1. Isolation Forest...")
start = time.time()
iso_forest = IsolationForest(
    contamination=contamination,
    random_state=RANDOM_STATE,
    n_estimators=100,
    max_samples='auto',
    n_jobs=-1
)
pred_iso = iso_forest.fit_predict(X_scaled)
outlier_predictions['Isolation Forest'] = (pred_iso == -1).astype(int)
execution_times['Isolation Forest'] = time.time() - start
print(f"   ‚úì Outliers: {(pred_iso == -1).sum():,} ({(pred_iso == -1).sum()/len(pred_iso)*100:.1f}%)")
print(f"   ‚úì Tempo: {execution_times['Isolation Forest']:.2f}s")

# 2. LOCAL OUTLIER FACTOR
print("\n2. Local Outlier Factor (LOF)...")
start = time.time()
lof = LocalOutlierFactor(
    contamination=contamination,
    n_neighbors=20,
    n_jobs=-1
)
pred_lof = lof.fit_predict(X_scaled)
outlier_predictions['LOF'] = (pred_lof == -1).astype(int)
execution_times['LOF'] = time.time() - start
print(f"   ‚úì Outliers: {(pred_lof == -1).sum():,} ({(pred_lof == -1).sum()/len(pred_lof)*100:.1f}%)")
print(f"   ‚úì Tempo: {execution_times['LOF']:.2f}s")

# 3. ONE-CLASS SVM
print("\n3. One-Class SVM...")
start = time.time()
oc_svm = OneClassSVM(
    nu=contamination,
    kernel='rbf',
    gamma='auto'
)
pred_ocsvm = oc_svm.fit_predict(X_scaled)
outlier_predictions['One-Class SVM'] = (pred_ocsvm == -1).astype(int)
execution_times['One-Class SVM'] = time.time() - start
print(f"   ‚úì Outliers: {(pred_ocsvm == -1).sum():,} ({(pred_ocsvm == -1).sum()/len(pred_ocsvm)*100:.1f}%)")
print(f"   ‚úì Tempo: {execution_times['One-Class SVM']:.2f}s")

# 4. ELLIPTIC ENVELOPE
print("\n4. Elliptic Envelope...")
start = time.time()
ee = EllipticEnvelope(
    contamination=contamination,
    random_state=RANDOM_STATE,
    support_fraction=0.9
)
pred_ee = ee.fit_predict(X_scaled)
outlier_predictions['Elliptic Envelope'] = (pred_ee == -1).astype(int)
execution_times['Elliptic Envelope'] = time.time() - start
print(f"   ‚úì Outliers: {(pred_ee == -1).sum():,} ({(pred_ee == -1).sum()/len(pred_ee)*100:.1f}%)")
print(f"   ‚úì Tempo: {execution_times['Elliptic Envelope']:.2f}s")

# 5. DBSCAN
print("\n5. DBSCAN (Clustering + Outliers)...")
start = time.time()
dbscan = DBSCAN(
    eps=3.0,
    min_samples=10,
    n_jobs=-1
)
pred_dbscan = dbscan.fit_predict(X_scaled)
outlier_predictions['DBSCAN'] = (pred_dbscan == -1).astype(int)
execution_times['DBSCAN'] = time.time() - start
n_clusters = len(set(pred_dbscan)) - (1 if -1 in pred_dbscan else 0)
print(f"   ‚úì Clusters: {n_clusters}")
print(f"   ‚úì Outliers: {(pred_dbscan == -1).sum():,} ({(pred_dbscan == -1).sum()/len(pred_dbscan)*100:.1f}%)")
print(f"   ‚úì Tempo: {execution_times['DBSCAN']:.2f}s")

# ============================================================================
# 4. VALIDA√á√ÉO CONTRA LABELS REAIS
# ============================================================================

print("\n[4/5] VALIDA√á√ÉO CONTRA LABELS REAIS")
print("-" * 80)

results = []

for method_name, y_pred in outlier_predictions.items():
    # M√©tricas
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    # Confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    results.append({
        'M√©todo': method_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'TP': tp,
        'FP': fp,
        'TN': tn,
        'FN': fn,
        'Tempo (s)': execution_times[method_name]
    })
    
    print(f"\n{method_name}:")
    print(f"  Accuracy:  {accuracy*100:.2f}%")
    print(f"  Precision: {precision*100:.2f}%")
    print(f"  Recall:    {recall*100:.2f}%")
    print(f"  F1-Score:  {f1*100:.2f}%")

# ============================================================================
# 5. AN√ÅLISE E EXPORTA√á√ÉO DOS RESULTADOS
# ============================================================================

print("\n[5/5] AN√ÅLISE E EXPORTA√á√ÉO DOS RESULTADOS")
print("-" * 80)

# Criar DataFrame com resultados
df_results = pd.DataFrame(results)
df_results = df_results.sort_values('F1-Score', ascending=False).reset_index(drop=True)
df_results['Rank'] = range(1, len(df_results) + 1)

print("\nRANKING DOS M√âTODOS (por F1-Score):")
print(df_results[['Rank', 'M√©todo', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Tempo (s)']].to_string(index=False))

# Melhor m√©todo
best_method = df_results.iloc[0]
print(f"\nüèÜ MELHOR M√âTODO: {best_method['M√©todo']}")
print(f"   Accuracy:  {best_method['Accuracy']*100:.2f}%")
print(f"   Precision: {best_method['Precision']*100:.2f}%")
print(f"   Recall:    {best_method['Recall']*100:.2f}%")
print(f"   F1-Score:  {best_method['F1-Score']*100:.2f}%")
print(f"   Tempo:     {best_method['Tempo (s)']:.2f}s")

# Salvar resultados
output_path = os.path.join(OUTPUT_DIR, 'unsupervised_results.csv')
df_results.to_csv(output_path, index=False)
print(f"\n‚úì Resultados salvos em: {output_path}")

# Estat√≠sticas gerais
print("\n" + "="*80)
print("ESTAT√çSTICAS GERAIS")
print("="*80)
print(f"M√©dia Accuracy:  {df_results['Accuracy'].mean()*100:.2f}%")
print(f"M√©dia Precision: {df_results['Precision'].mean()*100:.2f}%")
print(f"M√©dia Recall:    {df_results['Recall'].mean()*100:.2f}%")
print(f"M√©dia F1-Score:  {df_results['F1-Score'].mean()*100:.2f}%")
print(f"Tempo total:     {df_results['Tempo (s)'].sum():.2f}s")

# Insights
print("\n" + "="*80)
print("INSIGHTS")
print("="*80)
print(f"‚úì {len(outlier_predictions)} t√©cnicas de detec√ß√£o de outliers avaliadas")
print(f"‚úì Valida√ß√£o contra {len(y_true):,} amostras com labels reais")
print(f"‚úì Melhor m√©todo: {best_method['M√©todo']} (F1: {best_method['F1-Score']*100:.1f}%)")

avg_precision = df_results['Precision'].mean()
avg_recall = df_results['Recall'].mean()

if avg_precision > 0.5:
    print(f"‚úì Boa precision m√©dia ({avg_precision*100:.1f}%) - outliers correspondem a amea√ßas")
else:
    print(f"‚ö† Precision baixa ({avg_precision*100:.1f}%) - muitos falsos positivos")

if avg_recall > 0.5:
    print(f"‚úì Boa recall m√©dia ({avg_recall*100:.1f}%) - maioria das amea√ßas detectadas")
else:
    print(f"‚ö† Recall baixo ({avg_recall*100:.1f}%) - muitas amea√ßas n√£o detectadas")

print("\n" + "="*80)
print("‚úÖ AN√ÅLISE N√ÉO-SUPERVISIONADA CONCLU√çDA COM SUCESSO!")
print("="*80)
print(f"\nAplicabilidade em Aprendizado Federado:")
print(f"  ‚Ä¢ Detec√ß√£o de agentes maliciosos em ambientes distribu√≠dos")
print(f"  ‚Ä¢ Identifica√ß√£o de atualiza√ß√µes suspeitas de modelos")
print(f"  ‚Ä¢ Mitiga√ß√£o de ataques por envenenamento de dados")
