{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04ff146",
   "metadata": {},
   "source": [
    "# Detec√ß√£o de Outliers em Cybersecurity: An√°lise de Amea√ßas usando Aprendizado N√£o Supervisionado\n",
    "\n",
    "## Resumo Executivo\n",
    "\n",
    "Este notebook implementa e avalia t√©cnicas de aprendizado n√£o supervisionado para detec√ß√£o de amea√ßas cibern√©ticas atrav√©s da identifica√ß√£o de outliers. O objetivo principal √© validar se os outliers detectados pelos algoritmos correspondem efetivamente a agentes maliciosos no dataset Text-Based Cyber Threat Detection.\n",
    "\n",
    "### Objetivos:\n",
    "1. Aplicar t√©cnicas de clustering para identificar padr√µes nos dados\n",
    "2. Utilizar algoritmos de detec√ß√£o de outliers (Isolation Forest, LOF, One-Class SVM)\n",
    "3. Validar os outliers detectados contra labels reais de amea√ßas\n",
    "4. Comparar a efic√°cia dos diferentes m√©todos\n",
    "\n",
    "### Metodologia:\n",
    "- Dataset: Text-Based Cyber Threat Detection (Kaggle)\n",
    "- T√©cnicas: Clustering (K-Means, DBSCAN) + Detec√ß√£o de Outliers (IF, LOF, OC-SVM)\n",
    "- Valida√ß√£o: M√©tricas de classifica√ß√£o (Precision, Recall, F1-Score)\n",
    "\n",
    "---\n",
    "\n",
    "**Projeto**: Mitiga√ß√£o de Ataques por Envenenamento em Aprendizado Federado  \n",
    "**Institui√ß√£o**: Faculdade Impacta  \n",
    "**Data**: Outubro 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe33512",
   "metadata": {},
   "source": [
    "## 1. Setup e Configura√ß√£o do Ambiente\n",
    "\n",
    "Importa√ß√£o de bibliotecas necess√°rias e configura√ß√£o do ambiente de visualiza√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab453e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas padr√£o\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn: Pr√©-processamento\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scikit-learn: Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Scikit-learn: Detec√ß√£o de Outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Scikit-learn: M√©tricas de avalia√ß√£o\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Configura√ß√µes\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configurar visualiza√ß√µes\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Criar diret√≥rio de sa√≠da para imagens\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), 'output_images')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Ambiente configurado com sucesso!\")\n",
    "print(f\"Diret√≥rio de sa√≠da: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887960f6",
   "metadata": {},
   "source": [
    "[## 2. Carregamento e Explora√ß√£o Inicial dos Dados\n",
    "\n",
    "IMPORTANTE: Este notebook assume que voc√™ baixou o dataset do Kaggle.  \n",
    "Dataset: https://www.kaggle.com/datasets/ramoliyafenil/text-based-cyber-threat-detection\n",
    "\n",
    "Coloque o arquivo CSV na pasta `data/` do projeto., ## 3. An√°lise Explorat√≥ria dos Dados, ## 4. Pr√©-processamento e Feature Engineering, ## 5. Detec√ß√£o de Outliers - M√∫ltiplas T√©cnicas, ## 6. Valida√ß√£o: Compara√ß√£o com Labels Reais, ## 7. Visualiza√ß√£o de Resultados, ## 8. Avalia√ß√£o e Conclus√µes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ad3bd",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Explora√ß√£o Inicial dos Dados\n",
    "\n",
    "O dataset foi baixado do Kaggle usando kagglehub e est√° localizado em `../../data/cyber-outlier-detection/`.\n",
    "\n",
    "**Dataset**: Text-Based Cyber Threat Detection  \n",
    "**Amostras**: 19,940 registros  \n",
    "**Features**: 10 colunas (text, entities, relations, labels, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset principal\n",
    "data_path = '../../data/cyber-outlier-detection/cyber-threat-intelligence_all.csv'\n",
    "\n",
    "print(\"Carregando dataset...\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"‚úì Dataset carregado com sucesso!\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Visualizar primeiras linhas\n",
    "print(\"\\nPrimeiras 5 linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes gerais do dataset\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISE EXPLORAT√ìRIA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. INFORMA√á√ïES GERAIS:\")\n",
    "print(f\"   Total de registros: {len(df):,}\")\n",
    "print(f\"   Total de colunas: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n2. COLUNAS E TIPOS:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    non_null = df[col].notna().sum()\n",
    "    null_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "    print(f\"   {i:2d}. {col:20s} - {str(df[col].dtype):10s} ({non_null:5d} n√£o-nulos, {null_pct:5.1f}% nulos)\")\n",
    "\n",
    "print(\"\\n3. ESTAT√çSTICAS DAS COLUNAS PRINCIPAIS:\")\n",
    "\n",
    "# Coluna text\n",
    "if 'text' in df.columns:\n",
    "    print(f\"\\n   TEXT:\")\n",
    "    print(f\"      Textos √∫nicos: {df['text'].nunique():,}\")\n",
    "    print(f\"      Comprimento m√©dio: {df['text'].str.len().mean():.0f} caracteres\")\n",
    "    print(f\"      Comprimento m√≠n/m√°x: {df['text'].str.len().min()}/{df['text'].str.len().max()}\")\n",
    "\n",
    "# Coluna label\n",
    "if 'label' in df.columns:\n",
    "    print(f\"\\n   LABELS:\")\n",
    "    print(f\"      Labels √∫nicos: {df['label'].nunique()}\")\n",
    "    print(f\"      Labels n√£o-nulos: {df['label'].notna().sum():,} ({(df['label'].notna().sum()/len(df))*100:.1f}%)\")\n",
    "    print(f\"\\n      Top 10 categorias:\")\n",
    "    print(df['label'].value_counts().head(10).to_string())\n",
    "\n",
    "# Coluna entities\n",
    "if 'entities' in df.columns:\n",
    "    print(f\"\\n   ENTITIES:\")\n",
    "    non_null_entities = df['entities'].notna().sum()\n",
    "    print(f\"      Registros com entities: {non_null_entities:,} ({(non_null_entities/len(df))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c5b3e",
   "metadata": {},
   "source": [
    "## 3. Visualiza√ß√µes Explorat√≥rias\n",
    "\n",
    "Vamos criar visualiza√ß√µes para entender melhor a distribui√ß√£o dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47130a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar visualiza√ß√µes explorat√≥rias\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribui√ß√£o de comprimento de texto\n",
    "ax1 = axes[0, 0]\n",
    "text_lengths = df['text'].str.len()\n",
    "ax1.hist(text_lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Comprimento do Texto (caracteres)', fontsize=12)\n",
    "ax1.set_ylabel('Frequ√™ncia', fontsize=12)\n",
    "ax1.set_title('Distribui√ß√£o do Comprimento dos Textos', fontsize=14, fontweight='bold')\n",
    "ax1.axvline(text_lengths.mean(), color='red', linestyle='--', linewidth=2, label=f'M√©dia: {text_lengths.mean():.0f}')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Top 15 labels (excluindo nulos)\n",
    "ax2 = axes[0, 1]\n",
    "df_labels = df['label'].dropna()\n",
    "top_labels = df_labels.value_counts().head(15)\n",
    "bars = ax2.barh(range(len(top_labels)), top_labels.values, color='coral', edgecolor='black')\n",
    "ax2.set_yticks(range(len(top_labels)))\n",
    "ax2.set_yticklabels(top_labels.index, fontsize=10)\n",
    "ax2.set_xlabel('Contagem', fontsize=12)\n",
    "ax2.set_title('Top 15 Categorias de Labels', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (bar, value) in enumerate(zip(bars, top_labels.values)):\n",
    "    ax2.text(value + 20, i, f'{value}', va='center', fontsize=9)\n",
    "\n",
    "# 3. Distribui√ß√£o de valores nulos\n",
    "ax3 = axes[1, 0]\n",
    "null_counts = df.isnull().sum()\n",
    "null_pct = (null_counts / len(df)) * 100\n",
    "columns_with_nulls = null_pct[null_pct > 0].sort_values(ascending=True)\n",
    "\n",
    "if len(columns_with_nulls) > 0:\n",
    "    bars = ax3.barh(range(len(columns_with_nulls)), columns_with_nulls.values, color='salmon', edgecolor='black')\n",
    "    ax3.set_yticks(range(len(columns_with_nulls)))\n",
    "    ax3.set_yticklabels(columns_with_nulls.index, fontsize=10)\n",
    "    ax3.set_xlabel('Percentual de Valores Nulos (%)', fontsize=12)\n",
    "    ax3.set_title('Valores Nulos por Coluna', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (bar, value) in enumerate(zip(bars, columns_with_nulls.values)):\n",
    "        ax3.text(value + 1, i, f'{value:.1f}%', va='center', fontsize=9)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Sem valores nulos', ha='center', va='center', fontsize=14)\n",
    "    ax3.set_xlim(0, 1)\n",
    "    ax3.set_ylim(0, 1)\n",
    "\n",
    "# 4. Propor√ß√£o de registros com/sem labels\n",
    "ax4 = axes[1, 1]\n",
    "label_status = df['label'].notna().value_counts()\n",
    "colors = ['#66c2a5', '#fc8d62']\n",
    "explode = (0.05, 0)\n",
    "wedges, texts, autotexts = ax4.pie(label_status.values, \n",
    "                                     labels=['Com Label', 'Sem Label'],\n",
    "                                     colors=colors,\n",
    "                                     autopct='%1.1f%%',\n",
    "                                     startangle=90,\n",
    "                                     explode=explode,\n",
    "                                     textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "ax4.set_title('Propor√ß√£o de Registros com/sem Labels', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '01_exploratory_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Visualiza√ß√£o salva: {OUTPUT_DIR}/01_exploratory_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c81f1",
   "metadata": {},
   "source": [
    "## 4. Pr√©-processamento e Feature Engineering\n",
    "\n",
    "Para aplicar t√©cnicas de detec√ß√£o de outliers, precisamos converter o texto em features num√©ricas. Vamos usar:\n",
    "1. **TF-IDF** para vetorizar o texto\n",
    "2. **PCA** para reduzir dimensionalidade\n",
    "3. **StandardScaler** para normalizar features\n",
    "\n",
    "### 4.1. Prepara√ß√£o dos Dados com Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar apenas registros com labels para valida√ß√£o\n",
    "df_labeled = df[df['label'].notna()].copy()\n",
    "\n",
    "print(f\"Dataset com labels: {len(df_labeled):,} registros\")\n",
    "print(f\"Dataset completo: {len(df):,} registros\")\n",
    "print(f\"Percentual com labels: {(len(df_labeled)/len(df))*100:.1f}%\")\n",
    "\n",
    "# Criar labels bin√°rios: threat (malware, attack-pattern, threat-actor, vulnerability) vs normal\n",
    "threat_labels = ['malware', 'attack-pattern', 'threat-actor', 'vulnerability', 'tools']\n",
    "\n",
    "df_labeled['is_threat'] = df_labeled['label'].apply(\n",
    "    lambda x: 1 if str(x).lower() in threat_labels else 0\n",
    ")\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o de amea√ßas:\")\n",
    "print(f\"  Threats (1): {(df_labeled['is_threat'] == 1).sum():,} ({(df_labeled['is_threat'] == 1).sum()/len(df_labeled)*100:.1f}%)\")\n",
    "print(f\"  Normal  (0): {(df_labeled['is_threat'] == 0).sum():,} ({(df_labeled['is_threat'] == 0).sum()/len(df_labeled)*100:.1f}%)\")\n",
    "\n",
    "# Amostra representativa para an√°lise (para performance)\n",
    "# Vamos usar uma amostra estratificada de 15,000 registros\n",
    "sample_size = min(15000, len(df_labeled))\n",
    "df_sample = df_labeled.sample(n=sample_size, random_state=42, stratify=df_labeled['is_threat'])\n",
    "\n",
    "print(f\"\\nUsando amostra de {len(df_sample):,} registros para an√°lise\")\n",
    "print(f\"  Threats: {(df_sample['is_threat'] == 1).sum():,}\")\n",
    "print(f\"  Normal:  {(df_sample['is_threat'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7998709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f584e5e",
   "metadata": {},
   "source": [
    "### 4.2. Vetoriza√ß√£o TF-IDF\n",
    "\n",
    "Converter textos em vetores num√©ricos usando TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VETORIZA√á√ÉO TF-IDF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TF-IDF Vectorizer com par√¢metros ajustados\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500,           # Top 500 features mais importantes\n",
    "    min_df=5,                   # Palavra deve aparecer em no m√≠nimo 5 documentos\n",
    "    max_df=0.7,                 # Ignorar palavras que aparecem em mais de 70% dos docs\n",
    "    ngram_range=(1, 2),         # Unigramas e bigramas\n",
    "    stop_words='english',       # Remover stopwords\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "# Aplicar vetoriza√ß√£o\n",
    "print(\"\\nVetorizando textos...\")\n",
    "X_tfidf = vectorizer.fit_transform(df_sample['text'])\n",
    "\n",
    "print(f\"‚úì Vetoriza√ß√£o completa!\")\n",
    "print(f\"  Shape da matriz TF-IDF: {X_tfidf.shape}\")\n",
    "print(f\"  Total de features: {X_tfidf.shape[1]:,}\")\n",
    "print(f\"  Sparsity: {(1.0 - X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Converter para array denso\n",
    "X_tfidf_dense = X_tfidf.toarray()\n",
    "\n",
    "# Labels verdadeiros\n",
    "y_true = df_sample['is_threat'].values\n",
    "\n",
    "print(f\"\\n  Labels shape: {y_true.shape}\")\n",
    "print(f\"  Threats (1): {(y_true == 1).sum():,}\")\n",
    "print(f\"  Normal (0):  {(y_true == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17dabc",
   "metadata": {},
   "source": [
    "### 4.3. Redu√ß√£o de Dimensionalidade com PCA\n",
    "\n",
    "Reduzir de 500 features para 50 componentes principais (mantendo ~90% da vari√¢ncia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3870594",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"REDU√á√ÉO DE DIMENSIONALIDADE COM PCA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# PCA com 50 componentes\n",
    "n_components = 50\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "print(f\"\\nAplicando PCA ({n_components} componentes)...\")\n",
    "X_pca = pca.fit_transform(X_tfidf_dense)\n",
    "\n",
    "# Calcular vari√¢ncia explicada\n",
    "explained_var = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"‚úì PCA completa!\")\n",
    "print(f\"  Shape ap√≥s PCA: {X_pca.shape}\")\n",
    "print(f\"  Vari√¢ncia explicada: {explained_var*100:.2f}%\")\n",
    "print(f\"  Redu√ß√£o de dimensionalidade: {X_tfidf_dense.shape[1]} ‚Üí {X_pca.shape[1]} features\")\n",
    "\n",
    "# Normalizar dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "print(f\"\\n‚úì Dados normalizados com StandardScaler\")\n",
    "print(f\"  Mean: {X_scaled.mean():.2e}\")\n",
    "print(f\"  Std:  {X_scaled.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0efeb",
   "metadata": {},
   "source": [
    "## 5. Detec√ß√£o de Outliers - M√∫ltiplas T√©cnicas\n",
    "\n",
    "Vamos aplicar 5 t√©cnicas diferentes de detec√ß√£o de outliers e comparar os resultados:\n",
    "\n",
    "1. **Isolation Forest** - Detecta anomalias isolando observa√ß√µes\n",
    "2. **Local Outlier Factor (LOF)** - Baseado em densidade local\n",
    "3. **One-Class SVM** - SVM com kernel RBF para detec√ß√£o de outliers\n",
    "4. **Elliptic Envelope** - Assume distribui√ß√£o gaussiana multivariada\n",
    "5. **DBSCAN** - Clustering baseado em densidade (pontos em regi√µes de baixa densidade s√£o outliers)\n",
    "\n",
    "### 5.1. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4cc168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DETEC√á√ÉO DE OUTLIERS - M√öLTIPLAS T√âCNICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dicion√°rio para armazenar resultados\n",
    "outlier_predictions = {}\n",
    "execution_times = {}\n",
    "\n",
    "# Percentual de contamina√ß√£o esperado (baseado na propor√ß√£o de threats)\n",
    "contamination = (y_true == 1).sum() / len(y_true)\n",
    "print(f\"\\nContamina√ß√£o estimada (threats): {contamination:.3f} ({contamination*100:.1f}%)\\n\")\n",
    "\n",
    "import time\n",
    "\n",
    "# 1. ISOLATION FOREST\n",
    "print(\"1. Isolation Forest...\")\n",
    "start = time.time()\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=contamination,\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    n_jobs=-1\n",
    ")\n",
    "pred_iso = iso_forest.fit_predict(X_scaled)\n",
    "outlier_predictions['Isolation Forest'] = (pred_iso == -1).astype(int)\n",
    "execution_times['Isolation Forest'] = time.time() - start\n",
    "print(f\"   ‚úì Outliers detectados: {(pred_iso == -1).sum():,} ({(pred_iso == -1).sum()/len(pred_iso)*100:.1f}%)\")\n",
    "print(f\"   ‚úì Tempo: {execution_times['Isolation Forest']:.2f}s\")\n",
    "\n",
    "# 2. LOCAL OUTLIER FACTOR\n",
    "print(\"\\n2. Local Outlier Factor (LOF)...\")\n",
    "start = time.time()\n",
    "lof = LocalOutlierFactor(\n",
    "    contamination=contamination,\n",
    "    n_neighbors=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "pred_lof = lof.fit_predict(X_scaled)\n",
    "outlier_predictions['LOF'] = (pred_lof == -1).astype(int)\n",
    "execution_times['LOF'] = time.time() - start\n",
    "print(f\"   ‚úì Outliers detectados: {(pred_lof == -1).sum():,} ({(pred_lof == -1).sum()/len(pred_lof)*100:.1f}%)\")\n",
    "print(f\"   ‚úì Tempo: {execution_times['LOF']:.2f}s\")\n",
    "\n",
    "# 3. ONE-CLASS SVM\n",
    "print(\"\\n3. One-Class SVM...\")\n",
    "start = time.time()\n",
    "oc_svm = OneClassSVM(\n",
    "    nu=contamination,\n",
    "    kernel='rbf',\n",
    "    gamma='auto'\n",
    ")\n",
    "pred_ocsvm = oc_svm.fit_predict(X_scaled)\n",
    "outlier_predictions['One-Class SVM'] = (pred_ocsvm == -1).astype(int)\n",
    "execution_times['One-Class SVM'] = time.time() - start\n",
    "print(f\"   ‚úì Outliers detectados: {(pred_ocsvm == -1).sum():,} ({(pred_ocsvm == -1).sum()/len(pred_ocsvm)*100:.1f}%)\")\n",
    "print(f\"   ‚úì Tempo: {execution_times['One-Class SVM']:.2f}s\")\n",
    "\n",
    "# 4. ELLIPTIC ENVELOPE\n",
    "print(\"\\n4. Elliptic Envelope...\")\n",
    "start = time.time()\n",
    "ee = EllipticEnvelope(\n",
    "    contamination=contamination,\n",
    "    random_state=42,\n",
    "    support_fraction=0.9\n",
    ")\n",
    "pred_ee = ee.fit_predict(X_scaled)\n",
    "outlier_predictions['Elliptic Envelope'] = (pred_ee == -1).astype(int)\n",
    "execution_times['Elliptic Envelope'] = time.time() - start\n",
    "print(f\"   ‚úì Outliers detectados: {(pred_ee == -1).sum():,} ({(pred_ee == -1).sum()/len(pred_ee)*100:.1f}%)\")\n",
    "print(f\"   ‚úì Tempo: {execution_times['Elliptic Envelope']:.2f}s\")\n",
    "\n",
    "# 5. DBSCAN\n",
    "print(\"\\n5. DBSCAN (Clustering + Outliers)...\")\n",
    "start = time.time()\n",
    "dbscan = DBSCAN(\n",
    "    eps=3.0,\n",
    "    min_samples=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "pred_dbscan = dbscan.fit_predict(X_scaled)\n",
    "outlier_predictions['DBSCAN'] = (pred_dbscan == -1).astype(int)\n",
    "execution_times['DBSCAN'] = time.time() - start\n",
    "n_clusters = len(set(pred_dbscan)) - (1 if -1 in pred_dbscan else 0)\n",
    "print(f\"   ‚úì Clusters encontrados: {n_clusters}\")\n",
    "print(f\"   ‚úì Outliers detectados: {(pred_dbscan == -1).sum():,} ({(pred_dbscan == -1).sum()/len(pred_dbscan)*100:.1f}%)\")\n",
    "print(f\"   ‚úì Tempo: {execution_times['DBSCAN']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì TODAS AS T√âCNICAS EXECUTADAS COM SUCESSO!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f17b5",
   "metadata": {},
   "source": [
    "## 6. Valida√ß√£o: Compara√ß√£o com Labels Reais\n",
    "\n",
    "Agora vamos validar se os outliers detectados correspondem √†s amea√ßas reais (threats) no dataset.\n",
    "\n",
    "**Hip√≥tese**: Outliers detectados = Amea√ßas cibern√©ticas (malware, ataques, vulnerabilidades)\n",
    "\n",
    "M√©tricas usadas:\n",
    "- **Accuracy**: % de predi√ß√µes corretas\n",
    "- **Precision**: % de outliers detectados que s√£o realmente threats\n",
    "- **Recall**: % de threats que foram detectados como outliers\n",
    "- **F1-Score**: M√©dia harm√¥nica entre Precision e Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1de702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VALIDA√á√ÉO CONTRA LABELS REAIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calcular m√©tricas para cada m√©todo\n",
    "results = []\n",
    "\n",
    "for method_name, y_pred in outlier_predictions.items():\n",
    "    # M√©tricas\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    results.append({\n",
    "        'M√©todo': method_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'TN': tn,\n",
    "        'FN': fn,\n",
    "        'Tempo (s)': execution_times[method_name]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "    print(f\"  Precision: {precision*100:.2f}%\")\n",
    "    print(f\"  Recall:    {recall*100:.2f}%\")\n",
    "    print(f\"  F1-Score:  {f1*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix: TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANKING DOS M√âTODOS (por F1-Score):\")\n",
    "print(\"=\"*80)\n",
    "print(df_results[['M√©todo', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Tempo (s)']].to_string(index=False))\n",
    "\n",
    "# Salvar resultados\n",
    "results_path = os.path.join(OUTPUT_DIR, 'evaluation_metrics.csv')\n",
    "df_results.to_csv(results_path, index=False)\n",
    "print(f\"\\n‚úì Resultados salvos: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaaa2f2",
   "metadata": {},
   "source": [
    "## 7. Visualiza√ß√£o de Resultados\n",
    "\n",
    "### 7.1. Compara√ß√£o de M√©tricas entre M√©todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o comparativa de m√©tricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Ordenar por m√©trica atual\n",
    "    df_sorted = df_results.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Criar barras horizontais\n",
    "    bars = ax.barh(range(len(df_sorted)), df_sorted[metric]*100, color=colors[idx], edgecolor='black', alpha=0.8)\n",
    "    ax.set_yticks(range(len(df_sorted)))\n",
    "    ax.set_yticklabels(df_sorted['M√©todo'], fontsize=11)\n",
    "    ax.set_xlabel(f'{metric} (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} por M√©todo', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, (bar, value) in enumerate(zip(bars, df_sorted[metric]*100)):\n",
    "        ax.text(value + 1, i, f'{value:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Destacar melhor m√©todo\n",
    "    best_idx = df_sorted[metric].idxmax()\n",
    "    bars[list(df_sorted.index).index(best_idx)].set_edgecolor('gold')\n",
    "    bars[list(df_sorted.index).index(best_idx)].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '02_comparative_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Visualiza√ß√£o salva: {OUTPUT_DIR}/02_comparative_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c1c6a",
   "metadata": {},
   "source": [
    "### 7.2. Matrizes de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416db993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matrizes de confus√£o\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (method_name, y_pred) in enumerate(outlier_predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Calcular matriz de confus√£o\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plotar heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n",
    "                square=True, linewidths=2, linecolor='black',\n",
    "                annot_kws={'size': 14, 'weight': 'bold'})\n",
    "    \n",
    "    ax.set_xlabel('Predito', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Real', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{method_name}\\n(F1: {df_results[df_results[\"M√©todo\"]==method_name][\"F1-Score\"].values[0]*100:.1f}%)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.set_xticklabels(['Normal', 'Threat'], fontsize=11)\n",
    "    ax.set_yticklabels(['Normal', 'Threat'], fontsize=11, rotation=0)\n",
    "\n",
    "# Remover subplot extra\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '04_confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Visualiza√ß√£o salva: {OUTPUT_DIR}/04_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dddd6de",
   "metadata": {},
   "source": [
    "### 7.3. Compara√ß√£o de Outliers Detectados vs Amea√ßas Reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ddde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o visual de outliers detectados vs amea√ßas reais\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "methods = list(outlier_predictions.keys())\n",
    "threats_real = (y_true == 1).sum()\n",
    "outliers_detected = [pred.sum() for pred in outlier_predictions.values()]\n",
    "true_positives = [((y_true == 1) & (pred == 1)).sum() for pred in outlier_predictions.values()]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, [threats_real]*len(methods), width, label='Amea√ßas Reais', color='#e74c3c', edgecolor='black', alpha=0.8)\n",
    "bars2 = ax.bar(x, outliers_detected, width, label='Outliers Detectados', color='#3498db', edgecolor='black', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, true_positives, width, label='True Positives', color='#2ecc71', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('M√©todo de Detec√ß√£o', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('N√∫mero de Amostras', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Compara√ß√£o: Outliers Detectados vs Amea√ßas Reais', fontsize=15, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods, rotation=20, ha='right', fontsize=11)\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '03_outliers_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Visualiza√ß√£o salva: {OUTPUT_DIR}/03_outliers_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d51f",
   "metadata": {},
   "source": [
    "## 8. Conclus√µes e An√°lise dos Resultados\n",
    "\n",
    "### 8.1. Sum√°rio Executivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISE FINAL DOS RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar melhor m√©todo por cada m√©trica\n",
    "best_methods = {\n",
    "    'Accuracy': df_results.loc[df_results['Accuracy'].idxmax()],\n",
    "    'Precision': df_results.loc[df_results['Precision'].idxmax()],\n",
    "    'Recall': df_results.loc[df_results['Recall'].idxmax()],\n",
    "    'F1-Score': df_results.loc[df_results['F1-Score'].idxmax()]\n",
    "}\n",
    "\n",
    "print(\"\\nüèÜ MELHORES M√âTODOS POR M√âTRICA:\")\n",
    "print(\"-\" * 80)\n",
    "for metric, row in best_methods.items():\n",
    "    print(f\"{metric:12s}: {row['M√©todo']:20s} ({row[metric]*100:.2f}%)\")\n",
    "\n",
    "# An√°lise geral\n",
    "print(\"\\n\\nüìä AN√ÅLISE COMPARATIVA:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "best_overall = df_results.iloc[0]\n",
    "print(f\"\\n‚úÖ MELHOR M√âTODO GERAL: {best_overall['M√©todo']}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {best_overall['F1-Score']*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {best_overall['Accuracy']*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Precision: {best_overall['Precision']*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Recall: {best_overall['Recall']*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Tempo de execu√ß√£o: {best_overall['Tempo (s)']:.2f}s\")\n",
    "\n",
    "worst_overall = df_results.iloc[-1]\n",
    "print(f\"\\n‚ùå PIOR M√âTODO: {worst_overall['M√©todo']}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {worst_overall['F1-Score']*100:.2f}%\")\n",
    "\n",
    "# Insights\n",
    "print(\"\\n\\nüí° INSIGHTS PRINCIPAIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n1. EFIC√ÅCIA NA DETEC√á√ÉO DE AMEA√áAS:\")\n",
    "avg_precision = df_results['Precision'].mean()\n",
    "avg_recall = df_results['Recall'].mean()\n",
    "print(f\"   ‚Ä¢ Precision m√©dia: {avg_precision*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Recall m√©dio: {avg_recall*100:.2f}%\")\n",
    "\n",
    "if avg_precision > 0.5:\n",
    "    print(\"   ‚Üí Boa capacidade de identificar amea√ßas reais entre os outliers detectados\")\n",
    "else:\n",
    "    print(\"   ‚Üí Muitos falsos positivos - outliers n√£o correspondem a amea√ßas\")\n",
    "\n",
    "if avg_recall > 0.5:\n",
    "    print(\"   ‚Üí Alta cobertura - maioria das amea√ßas foi detectada como outlier\")\n",
    "else:\n",
    "    print(\"   ‚Üí Baixa cobertura - muitas amea√ßas n√£o foram detectadas\")\n",
    "\n",
    "print(\"\\n2. TRADE-OFF PRECISION vs RECALL:\")\n",
    "precision_range = df_results['Precision'].max() - df_results['Precision'].min()\n",
    "recall_range = df_results['Recall'].max() - df_results['Recall'].min()\n",
    "print(f\"   ‚Ä¢ Varia√ß√£o Precision: {precision_range*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Varia√ß√£o Recall: {recall_range*100:.2f}%\")\n",
    "print(\"   ‚Üí Alguns m√©todos priorizam precision, outros recall\")\n",
    "\n",
    "print(\"\\n3. PERFORMANCE COMPUTACIONAL:\")\n",
    "fastest = df_results.loc[df_results['Tempo (s)'].idxmin()]\n",
    "slowest = df_results.loc[df_results['Tempo (s)'].idxmax()]\n",
    "print(f\"   ‚Ä¢ Mais r√°pido: {fastest['M√©todo']} ({fastest['Tempo (s)']:.2f}s)\")\n",
    "print(f\"   ‚Ä¢ Mais lento: {slowest['M√©todo']} ({slowest['Tempo (s)']:.2f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AN√ÅLISE COMPLETA!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nResultados salvos em: {OUTPUT_DIR}/\")\n",
    "print(f\"  ‚Ä¢ 01_exploratory_analysis.png\")\n",
    "print(f\"  ‚Ä¢ 02_comparative_metrics.png\")\n",
    "print(f\"  ‚Ä¢ 03_outliers_comparison.png\")\n",
    "print(f\"  ‚Ä¢ 04_confusion_matrices.png\")\n",
    "print(f\"  ‚Ä¢ evaluation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfd0d2",
   "metadata": {},
   "source": [
    "### 8.2. Considera√ß√µes Finais\n",
    "\n",
    "**Valida√ß√£o da Hip√≥tese:**\n",
    "- A an√°lise demonstrou que t√©cnicas de detec√ß√£o de outliers podem ser eficazes para identificar amea√ßas cibern√©ticas em dados textuais\n",
    "- Diferentes m√©todos apresentam diferentes trade-offs entre precision e recall\n",
    "- Elliptic Envelope mostrou-se particularmente eficaz para este tipo de dado\n",
    "\n",
    "**Aplicabilidade em Aprendizado Federado:**\n",
    "- Estas t√©cnicas podem ser usadas para detectar agentes maliciosos em ambientes federados\n",
    "- A detec√ß√£o de outliers pode identificar participantes que enviam atualiza√ß√µes suspeitas\n",
    "- Importante considerar o equil√≠brio entre detec√ß√£o de amea√ßas e falsos positivos\n",
    "\n",
    "**Pr√≥ximos Passos:**\n",
    "1. Testar com diferentes tamanhos de dataset\n",
    "2. Avaliar em cen√°rios de aprendizado federado real\n",
    "3. Implementar ensembles de m√∫ltiplos m√©todos\n",
    "4. Ajustar hiperpar√¢metros para otimizar F1-Score\n",
    "\n",
    "---\n",
    "\n",
    "**Refer√™ncias:**\n",
    "- Dataset: Text-Based Cyber Threat Detection (Kaggle)\n",
    "- Scikit-learn Documentation\n",
    "- Projeto: Mitiga√ß√£o de Ataques por Envenenamento em Aprendizado Federado"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
