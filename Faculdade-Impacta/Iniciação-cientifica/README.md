# MITIGAÃ‡ÃƒO DE ATAQUES POR ENVENENAMENTO EM APRENDIZADO FEDERADO

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-ML-orange.svg)
![License](https://img.shields.io/badge/License-Academic-green.svg)

## ğŸ“‹ Sobre o Projeto

Este projeto de IniciaÃ§Ã£o CientÃ­fica investiga estratÃ©gias de **detecÃ§Ã£o de outliers** para mitigar **ataques por envenenamento** em sistemas de **Aprendizado Federado** (Federated Learning - FL). 

O aprendizado federado Ã© um paradigma emergente de machine learning distribuÃ­do que permite treinamento colaborativo de modelos preservando a privacidade dos dados. No entanto, sua natureza descentralizada o torna vulnerÃ¡vel a **ataques maliciosos de envenenamento**, onde agentes adversÃ¡rios manipulam o processo de treinamento enviando atualizaÃ§Ãµes corrompidas.

Este estudo combina **revisÃ£o integrativa da literatura** com **simulaÃ§Ãµes computacionais** para avaliar tÃ©cnicas defensivas baseadas em **detecÃ§Ã£o de anomalias**, contribuindo para o fortalecimento da seguranÃ§a em sistemas de ML distribuÃ­do.

---

## ğŸ¯ Objetivos

### Objetivo Geral

Investigar e avaliar estratÃ©gias de prevenÃ§Ã£o e mitigaÃ§Ã£o de ataques de envenenamento em sistemas de aprendizado federado.

### Objetivos EspecÃ­ficos

1. Analisar vulnerabilidades do FL a ataques por envenenamento
2. Investigar mÃ©todos baseados em outliers para detecÃ§Ã£o de agentes maliciosos
3. Simular cenÃ¡rios controlados de ataque e defesa
4. Validar abordagens defensivas atravÃ©s de mÃ©tricas quantitativas

---

## ğŸ—‚ï¸ Estrutura do RepositÃ³rio

```plaintext
.
â”œâ”€â”€ README.md                         # DocumentaÃ§Ã£o principal
â”œâ”€â”€ requirements.txt                  # DependÃªncias Python
â”‚
â”œâ”€â”€ code/                             # CÃ³digo-fonte dos experimentos
â”‚   â”œâ”€â”€ iris-dataset/
â”‚   â”œâ”€â”€ penguin-dataset/
â”‚   â””â”€â”€ weight_height/
â”‚
â”œâ”€â”€ data/                             # Datasets utilizados
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter Notebooks e anÃ¡lises
â”‚   â”œâ”€â”€ cyber-threat-detection/
â”‚   â”œâ”€â”€ iris/
â”‚   â”œâ”€â”€ penguin/
â”‚   â””â”€â”€ weight_height/
â”‚
â””â”€â”€ docs/                             # DocumentaÃ§Ã£o tÃ©cnica completa
    â”œâ”€â”€ Projeto.md
    â”œâ”€â”€ aprendizado-supervisionado/
    â”œâ”€â”€ aprendizado-nÃ£o-supervisionado/
    â”œâ”€â”€ avaliaÃ§Ãµes-de-modelos/
    â”œâ”€â”€ desafios/
    â””â”€â”€ detecÃ§Ã£o-de-outliers/
```

---

## ï¿½ Metodologia

O projeto adota uma abordagem mista combinando pesquisa teÃ³rica e experimental:

### 1. RevisÃ£o Integrativa da Literatura

RevisÃ£o sistemÃ¡tica sobre ataques de envenenamento em aprendizado federado e tÃ©cnicas de mitigaÃ§Ã£o baseadas em detecÃ§Ã£o de outliers.

### 2. SimulaÃ§Ãµes Computacionais

ImplementaÃ§Ã£o de experimentos controlados utilizando mÃºltiplos datasets para avaliar tÃ©cnicas de detecÃ§Ã£o de anomalias em diferentes contextos:

- **Datasets de Benchmark**: Iris, Penguins, Weight-Height
- **Dataset de Cybersecurity**: DetecÃ§Ã£o de ameaÃ§as cibernÃ©ticas
- **TÃ©cnicas de ML**: Supervisionado, NÃ£o-supervisionado, DetecÃ§Ã£o de Outliers
- **MÃ©tricas**: Accuracy, Precision, Recall, F1-Score, Silhouette, Davies-Bouldin

### 3. ValidaÃ§Ã£o Experimental

VerificaÃ§Ã£o da eficÃ¡cia das tÃ©cnicas atravÃ©s da comparaÃ§Ã£o entre outliers detectados e agentes maliciosos conhecidos.

---

## ï¿½ Principais Resultados

Este projeto demonstra que **tÃ©cnicas de detecÃ§Ã£o de outliers** sÃ£o eficazes para identificar **agentes maliciosos** em sistemas de aprendizado federado.

### TÃ©cnicas de DetecÃ§Ã£o Avaliadas

- **Isolation Forest**: DetecÃ§Ã£o baseada em isolamento aleatÃ³rio
- **Local Outlier Factor (LOF)**: AnÃ¡lise de densidade local
- **One-Class SVM**: Fronteira de decisÃ£o em alta dimensÃ£o
- **Elliptic Envelope**: Modelo gaussiano multivariado
- **DBSCAN**: Clustering baseado em densidade

### Performance em DetecÃ§Ã£o de AmeaÃ§as

| TÃ©cnica | Accuracy | Precision | Recall | F1-Score |
|---------|----------|-----------|--------|----------|
| **Elliptic Envelope** | **99.52%** | **97.62%** | **99.52%** | **98.56%** |
| Isolation Forest | 97.14% | 85.71% | 97.14% | 91.09% |
| LOF | 95.24% | 80.00% | 95.24% | 86.96% |
| One-Class SVM | 90.48% | 65.52% | 90.48% | 76.00% |
| DBSCAN | 85.71% | 55.56% | 85.71% | 67.57% |

**ConclusÃ£o**: As tÃ©cnicas avaliadas demonstraram alta eficÃ¡cia (85-99% de acurÃ¡cia) na identificaÃ§Ã£o de agentes maliciosos, validando a aplicabilidade de detecÃ§Ã£o de outliers como mecanismo de defesa em aprendizado federado.

---

## ğŸ› ï¸ Tecnologias e Ferramentas

- **Linguagem**: Python 3.8+
- **Machine Learning**: scikit-learn, pandas, numpy
- **VisualizaÃ§Ã£o**: matplotlib, seaborn
- **Ambiente**: Jupyter Notebook, VS Code
- **Controle de VersÃ£o**: Git/GitHub

## ğŸš€ Como Executar

### PrÃ©-requisitos

```bash
Python 3.8+
pip
```

### InstalaÃ§Ã£o

```powershell
# Clone o repositÃ³rio
git clone https://github.com/Ryanditko/IC-aprendizado-federado-e-machine-learning-em-cybersecurity.git
cd IC-aprendizado-federado-e-machine-learning-em-cybersecurity

# Instale as dependÃªncias
pip install -r requirements.txt
```

### ExecuÃ§Ã£o

**Notebooks Jupyter:**

```powershell
jupyter notebook
```

**Scripts Python:**

```powershell
# Exemplo: anÃ¡lise supervisionada do Iris
python code/iris-dataset/aprendizado-supervisionado.py

# Exemplo: detecÃ§Ã£o de outliers
python notebooks/cyber-threat-detection/cyber_threat_outlier_detection.py
```

---

## ğŸ“š DocumentaÃ§Ã£o Completa

Para documentaÃ§Ã£o tÃ©cnica detalhada, consulte a pasta `docs/`:

- **[Projeto.md](docs/Projeto.md)**: Documento oficial do projeto
- **[DetecÃ§Ã£o de Outliers](docs/detecÃ§Ã£o-de-outliers/)**: FundamentaÃ§Ã£o teÃ³rica e aplicaÃ§Ã£o em FL
- **[AvaliaÃ§Ã£o de Modelos](docs/avaliaÃ§Ãµes-de-modelos/)**: Metodologias e resultados experimentais

## ğŸ” AplicaÃ§Ã£o em Aprendizado Federado

### Problema

Em sistemas de aprendizado federado, **clientes maliciosos** podem enviar **atualizaÃ§Ãµes corrompidas** que comprometem o modelo global, caracterizando um **ataque de envenenamento**.

### SoluÃ§Ã£o Proposta

**Pipeline de Defesa:**

1. Coleta de atualizaÃ§Ãµes dos clientes participantes
2. **DetecÃ§Ã£o de outliers** nas atualizaÃ§Ãµes recebidas
3. Filtragem de agentes suspeitos
4. AgregaÃ§Ã£o robusta dos gradientes legÃ­timos

### TÃ©cnicas Validadas

- **Elliptic Envelope**: 99.52% de acurÃ¡cia na detecÃ§Ã£o
- **Isolation Forest**: 97.14% de acurÃ¡cia
- **Byzantine-Robust Aggregation**: Complemento Ã s tÃ©cnicas de outliers

## ğŸ¤ ContribuiÃ§Ãµes CientÃ­ficas

Este projeto contribui para o avanÃ§o do conhecimento em:

- **SeguranÃ§a em Aprendizado Federado**: Mapeamento de tÃ©cnicas de mitigaÃ§Ã£o
- **DetecÃ§Ã£o de Anomalias**: AvaliaÃ§Ã£o comparativa de mÃ©todos
- **CiberseguranÃ§a**: Diretrizes para implementaÃ§Ã£o segura de ML distribuÃ­do

---

## ğŸ‘¤ InformaÃ§Ãµes do Projeto

**Tipo**: IniciaÃ§Ã£o CientÃ­fica  
**InstituiÃ§Ã£o**: Faculdade Impacta  
**Ãrea**: CiÃªncia da ComputaÃ§Ã£o / CiberseguranÃ§a / Machine Learning

## ğŸ“„ LicenÃ§a

Este projeto Ã© de natureza acadÃªmica e destinado a fins educacionais e de pesquisa cientÃ­fica.

---

**Temas**: SeguranÃ§a | Machine Learning | Federated Learning | Data Science

Projeto desenvolvido no Ã¢mbito do programa de IniciaÃ§Ã£o CientÃ­fica
